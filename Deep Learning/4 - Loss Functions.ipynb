{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss/Cost Function and types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning (Artifical Neural Networks) is divided into two categories: \n",
    "\n",
    "1. Regression (Output will be a continous value)\n",
    "\n",
    "2. Classification (Output will be a categorical value)\n",
    "\n",
    "Different loss/cost functions are used for both Regression and Classification. Before proceeding, its imporant to understand the difference between Loss and Cost functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference between Loss and Cost Function\n",
    "\n",
    "For example, lets consider a neural network where we need to train with 100 records. But everytime, we pass one by one record then calculate the loss function and repeat. This task is completely cumbersome. Hence, to avoid this we can pass 10 records in a single batch likwise within 10 times we can train our Neural network completely. Here, we will use the Cost function. \n",
    "\n",
    "In simple words, \n",
    "\n",
    "| Loss Function | Cost Function |\n",
    "| :---: | :---: |\n",
    "| It will be applicable only when record in given as input | It will be applicable when a batch of records are passed together |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Loss/Cost Functions w.r.t Regression\n",
    "\n",
    "Since regression provides output in the form of Continous, the loss or cost functions used should support the same. There are three types which are mostly used. They are: \n",
    "\n",
    "1. Mean Squared Error (MSE)\n",
    "\n",
    "2. Mean Absolute Error (MAE)\n",
    "3. Huber Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ Loss Function = 1/2 * (\\ y - \\hat{y})^2 $ \n",
    "\n",
    "The equation of loss function is a Quadratic Equation. Plotting the quadratic equation gives us a Gradient descent (U - shaped curve)\n",
    "\n",
    "$\\displaystyle Cost Function = 1/2 * \\sum_{i=1}^n (\\ y - \\hat{y})^2 $\n",
    "\n",
    "**Advantages:**\n",
    "- Differentiable\n",
    "- It has only one local/global minima\n",
    "- It converges faster\n",
    "\n",
    "**Disadvantages:**\n",
    "- Not robust to outliers (Best fit line gets shifted because of outliers, penalizing the error by squaring the outlier value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Mean Absolute Error (MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ Loss Function = 1/2 * (y - \\hat{y}) $\n",
    "\n",
    "$\\displaystyle Cost Function = 1/2 * \\sum_{i=1}^n (\\ y - \\hat{y}) $\n",
    "\n",
    "**Advantages:**\n",
    "- Robust to outliers (Here we are not squaring the outlier value)\n",
    "\n",
    "**Disadvantages:**\n",
    "- Time Consuming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Huber Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combination of Mean Squared Error (MSE) and Mean Absolute Error (MAE)\n",
    "\n",
    "We will use the help of hyperparameter $\\displaystyle \\delta$. The Huber loss function, revolves around two conditions:\n",
    "\n",
    "**Loss Function if there are no outliers**:\n",
    "\n",
    "$\\displaystyle Loss Function = 1/2 * (y - \\hat{y})^2 $\n",
    "\n",
    "if $\\ |y- \\hat{y}| \\leq \\delta $\n",
    "\n",
    "\n",
    "**Loss Function if there are outliers**:\n",
    "\n",
    "$\\displaystyle Loss Function = \\delta |y - \\hat{y}| - 1/2 * \\delta ^2 $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Loss/Cost Function w.r.t Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since classification provides output in the form of categorical variable, the loss or cost functions used should support the same. There are three types which are mostly used. They are: \n",
    "\n",
    "1. Binary Cross Entropy (for Binary Classification)\n",
    "\n",
    "2. Categorical Cross Entropy (for Multiclass Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Binary Cross Entropy a.k.a Log loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, its a Binary classification we need to use the Sigmoid Activation Function at the Output layer. This only works when the output has only two classes: 0 and 1.\n",
    "\n",
    "Below image is the loss function:\n",
    "\n",
    "![Image](https://arize.com/wp-content/uploads/2022/11/log-loss-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Categorical Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This entropy is calculated for the multi classification problems where the output classes are more than 2. Since, its a multi-class classification, Softmax activation function should be used.\n",
    "\n",
    "$\\ Loss(x_i, y_i) = - \\sum_{j=1}^c y_{ij} * ln (\\hat {y_{ij}}) $\n",
    "\n",
    "where\n",
    "\n",
    "- $\\ y_i = [y_{i1}, y_{i2}, y_{i3}, ..., y_{ic}]$\n",
    "\n",
    "- $\\ y_{ij} $ = {1 if the element is in class, 0 otherwise}\n",
    "- $\\displaystyle \\hat{y_{ij}} $ is the Softmax Activation Function (applied on O/P Layer since its a Multi-class classification)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
