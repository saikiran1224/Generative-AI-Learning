{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function and types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation Function is a function where in which used in the Hidden layer neurons which will basically says how much the neuron should get activated or deactivated. Lets explore the various type of activation function which are most used: \n",
    "\n",
    "1. Sigmoid \n",
    "\n",
    "2. Tanhx \n",
    "3. ReLU (Rectified Linear Unit)\n",
    "4. Leaky ReLU\n",
    "5. Pre ReLU\n",
    "6. Softmax  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is defined as $\\displaystyle z = \\sigma(x) = \\frac {1}{1 + e^{-x}} $. \n",
    "\n",
    "Value of z lies between 0 and 1. The derivative of Sigmoid function lies between 0 and 0.25, due to which Vanishing Gradient problem exists. \n",
    "\n",
    "**Advantages:**\n",
    "- Smooth Gradient\n",
    "\n",
    "- Output values bound between 0 and 1, normalizing the values.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "- Prone to Vanishing gradient/Gradient Vanishing\n",
    "\n",
    "- Function output is Zero centered\n",
    "- Calculation of exponential values increases the time complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tanhx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is defined as $\\displaystyle z = \\frac {e^x - e^{-x}}{e^x + e^{-x}} $\n",
    "\n",
    "Value of tanhx lies between -1 to 1. The derivative of tanhx lies between 0 to 1. \n",
    "\n",
    "Unlike Sigmoid function, tanhx is a Zero Centered Curve (it means the curve passes through 0 when plotted in the graph and lies between -1 to 1). \n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Zero-centered curve\n",
    "\n",
    "- Output values bound between -1 to 1. \n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "- Still prone to Vanishing gradient \n",
    "\n",
    "- More time complexity as exponential operations are included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ReLU (Rectified Linear Unit) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most popular activation function which is used by the researchers. Denoted by \n",
    "\n",
    "$\\ ReLU = max(0, x) $ where $\\ x $ is the Input\n",
    "\n",
    "The simple definition of ReLU states two main things: \n",
    "- If x < 0 or x = 0, the final value of ReLU will be 0. \n",
    "- If x > 0, then final value of ReLU will be x. \n",
    "\n",
    "The derivative of ReLU activation function is either 0 or 1.\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- ReLU is much more quicker than other activation functions since its just a $\\ max() $ function. \n",
    "\n",
    "- Solves the Vanishing Gradient problem.\n",
    "\n",
    "**Disadvantage:**\n",
    "\n",
    "- Since the value is either 0 or 1 (derivative), consider if the value is 0 then the value of derivative in weights updation formula will be 0 making old weight again equal to the new weight. \n",
    "\n",
    "- **Once a Negative number is entered, ReLU will die. This is termed as Dead ReLU**\n",
    "- ReLU function is not a zero-centric function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Leaky ReLU "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is similar to ReLU. But this Leaky ReLU Activation function solves the Dead ReLU. \n",
    "\n",
    "There will be a small change in the ReLU function. Instead of the 0 in the first half, we will replace it by $\\ 0.01 * x $.\n",
    "\n",
    "The Final function used is $\\ Leaky ReLU = max(0.01x, x) $\n",
    "\n",
    "**Note:** Its not fully proved that Leaky ReLU is better than ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pre ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre ReLU is a combination of ReLU and Leaky ReLU. Combining both the activation functions, there is a equation which is depicted as follows:\n",
    "\n",
    "If $\\ y_i > 0 $ then $\\ f(y_i) = y_i $\n",
    "\n",
    "If $ y_i \\leq 0 $ then $\\ f(y_i) = a_i * y_i $ \n",
    "\n",
    "Observations:\n",
    "\n",
    "- If $\\ a_i = 0 $, then it becomes ReLU\n",
    "\n",
    "- If $\\ a_i > 0 $, then it becomes Leaky ReLU\n",
    "- If $\\ a_i $ is learnable parameter, then it becomes Pre ReLU\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all the multi-classification problems in the output layer, Softmax activation function is used. \n",
    "\n",
    "${\\displaystyle \\sigma (\\mathbf {z} )_{i}={\\frac {e^{z_{i}}}{\\sum _{j=1}^{K}e^{z_{j}}}}}$\n",
    "\n",
    "where: \n",
    "\n",
    "- $ \\sigma $\t=\tsoftmax\n",
    "\n",
    "- $\\vec{z} $\t=\tinput vector\n",
    "- $\\ {e^{z_{i}}} $\t=\tstandard exponential function for input vector\n",
    "- $\\ K $\t=\tnumber of classes in the multi-class classifier\n",
    "- $\\ e^{z_{j}} $\t=\tstandard exponential function for output vector\n",
    "- $\\ e^{z_{j}} $\t=\tstandard exponential function for output vector $\n",
    "\n",
    "\n",
    "**The number of nodes in the output layer = The number of output classes N**\n",
    "\n",
    "The speciality of this activation function, once we receive the raw output from the neural network, then Softmax activation function converts them into a vector of probabilities (array representation). \n",
    "\n",
    "### Example\n",
    "\n",
    "For example, our output has three classes - [Good, Bad, Neutral]\n",
    "\n",
    "If the Neural network gives an output as \"Good\", then the representation of this will be [1, 0, 0]\n",
    "Similarly, if the output is \"Bad\", then the representation will be in the form of [0, 1, 0] and if the output is \"Neutral\" the final output will be [0, 0, 1]\n",
    "\n",
    "Likewise, this activation function converts the output in the encoding form as mentioned which will be later used to calculate the probabilties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which Activation function should be used when?\n",
    "\n",
    "There were many types of activation functions, but its crucial to know which Activation function should be used when to achieve desired results. Lets understand in terms of Regression and Classification (Binary and Multi-class)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our problem is related to Regression, then the following activation function should be used at Hidden layer and Output layers:\n",
    "\n",
    "**Hidden Layer:** ReLU Activation function\n",
    "\n",
    "**Output Layer:** Linear Activation function (output should be in continous variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our problem is related to Classification, we will discuss about the activation function which should be used for both Binary and Multi-class classification. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hidden Layer:** ReLU Activation function\n",
    "\n",
    "**Output Layer:** Sigmoid Activation Function (output lies between 0 to 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hidden Layer:** ReLU Activation function\n",
    "\n",
    "**Output Layer:** Softmax Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### In all the types, ReLU activation function will be applied on the hidden layer whereas the activation function applied on the Output layer varies."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
