{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main aim of optimizers is to reduce the value of loss function making it near to 0. As part of Back propogation, first we will calculate the loss function to estimate the error in output, and then use the help of optimizers and finally update the weights. \n",
    "\n",
    "There are different kinds of optimizers introduced, but among of them **Adam Optimizer** stands best. Lets explore each and every optimizer in detail along with the advantages and disadvantages of each. \n",
    "\n",
    "But, before learning about various optimizers we will learn about **Epoch**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing Epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch is defined as one complete cycle of Forward and Backward propogation. \n",
    "\n",
    "Eg. In a Neural network, once the inputs are passed forward propogation happens by applying weights, calculating the summation of product of weights and inputs, and passing to Activation function and finally predicting the output, calculating the loss function, using optimizer to reduce the value to 0, updation of weights. \n",
    "\n",
    "This whole operations constitute one Epoch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Linear Regression, we have used the help of Gradient Descent to find the best fit line. Similarly, here once the output is calculated, Loss function is calculated to find the percentage of error. \n",
    "\n",
    "To reduce the error percentage, Optimizers are used. Gradient Descent is a optimizer in which we will use the MSE (Mean Squared Error) as the cost function. \n",
    "\n",
    "In the weight updation formula, \n",
    "\n",
    "$\\displaystyle w_{new} = w_{old} - \\eta * (\\displaystyle \\frac {\\partial Loss}{\\partial w_{old}}) $\n",
    "\n",
    "$ \\eta$ is the learning rate which helps us to reach the global minima or reaching to the convergence in the Gradient Descent. \n",
    "\n",
    "Here, in forward propogation if we pass 1 Million records at one time in a batch, then the cost function calculates the summation of difference between predicted and actual value for all the 1 Million records at a time which is highly cost intensive. In one epoch, 1 Million records will be passed at once.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "- High amount of RAM is required to store all the 1 Million jobs at a time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to improve the Gradient Descent optimizer drawback, in Stochastic Gradient Descent, we will pass one record at a time instead of passing all of them in one batch like Gradient Descent. \n",
    "\n",
    "For each record in 1 Million, 1 individual Epoch at a time will execute until all of them gets completed.\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- No need to high RAM capacity, since one record is loaded every time.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "- Convergence will be very slow in gradient descent to reach global minima\n",
    "- Very high time complexity\n",
    "- High noise (Zig zag lines) in gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mini Batch Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its a slight improvement of Stochastic Gradient Descent, where instead of sending one record at a time, we will divide the whole 1 Million records into some mini batches. For example, for every iteration/mini batch we will send 1000 records at a time. \n",
    "\n",
    "In one epoch, 1000 records will be sent until all of them gets completed. \n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Resource Intensive\n",
    "- Convergence will be better compared to SGD.\n",
    "- Better time complexity\n",
    "- Less Noise (Less zig xag lines) compared to SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stochastic Gradient Descent with Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the noise (zig zag lines) in the gradient descent, We use the help of Exponential Weighted average where we use a formula where the derivative in the weight updation formula gets replaced with $\\ V_{dw}$. \n",
    "\n",
    "$\\displaystyle w_{t} = w_{t-1} - \\eta * V_{dw} $\n",
    "\n",
    "\n",
    "Using the help of $\\ V_{dw}$  and $\\displaystyle w_{t}$ time series, it helps us to smoothen the curve.\n",
    "\n",
    "$\\ V_{dw} = \\beta * V_{dw} + (1-\\beta) * (\\displaystyle \\frac {\\partial Loss}{\\partial w_{old}})^2$\n",
    "\n",
    "where $\\ \\beta$ is the Learning rate.\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Reduces the noise\n",
    "- Solves the problem of mini batch\n",
    "- Provides quick convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Adaptive Gradient Descent (Adgrad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word adaptive means here the $\\ \\eta$ can be controlled like either we can increase or decrease based on reaching towards the global minima.\n",
    "\n",
    "As discussed in the Gradient Descent, we use the help of $\\ \\eta$ to reach towards the global minima in the weight updation formula. Lets use the weight updation formula in respective of time, then the formula looks like as below: \n",
    "\n",
    "$\\displaystyle w_{t} = w_{t-1} - \\eta * (\\displaystyle \\frac {\\partial Loss}{\\partial w_{t-1}}) $\n",
    "\n",
    "But here instead of using the $\\ \\eta$ to modify the learning rate, we will replace with $\\ \\eta^{ |}$\n",
    "\n",
    "$\\ \\eta^{ |}$ is defined as the ratio of $\\ \\eta$ to square root of sum of $\\displaystyle \\alpha_t + \\epsilon$. \n",
    "\n",
    "Here $ \\epsilon$ is a small number used which makes sure the denominator is not equal to 0, and $\\displaystyle \\alpha_t$ is summation of derivative of Loss function w.r.t previous time. \n",
    "\n",
    "Note: t(timestamp) means the wieight at a particulat timestamp.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "- In $\\displaystyle \\alpha_t$, the value some times can be very high resulting the $\\ \\eta$ change making it very negligible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Adadelta and RMS prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It solves the problem of $\\displaystyle \\alpha_t$ getting high. \n",
    "\n",
    "The formula for the $\\ \\eta^{ |}$ will be modified. Its defined as the square root of the $\\displaystyle Sdw + \\epsilon $ \n",
    "\n",
    "where $\\displaystyle Sdw = \\beta * Sdw_{t-1} + (1-\\beta) * (\\displaystyle \\frac {\\partial Loss}{\\partial w_{t-1}})^2$\n",
    "\n",
    "Here, we will use the help of $\\displaystyle \\beta$ to control the learning rate either increase or decrease in the derivative whereas in the Adgrad or AGD, we are not able to do as its the summation of all the derivatives.\n",
    "\n",
    "**Advantages:**\n",
    "- Able to control the learning rate, hence controlling the increase in derivative dynamically as per the requirement. \n",
    "\n",
    "**Disadvantages:**\n",
    "- Smoothening of the curve is not done which was acheived in SGD with Momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Adam Optimizers (SGD with momentum + RMS prop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the **Best optimizer** which is currently in use with all the deep learning researchers. \n",
    "\n",
    "Here, we will use the power of SGD with Momentum (Smoothening of the curve) + RMS prop (Control the learning rate) at one place. The formula is also modified accordingly as below: \n",
    "\n",
    "$\\displaystyle w_{t} = w_{t-1} - \\eta^{|} * (\\displaystyle Vdw_{t}) $\n",
    "\n",
    "$\\displaystyle b_{t} = b_{t-1} - \\eta^{|} * (\\displaystyle Vdb_{t}) $\n",
    "\n",
    "where: \n",
    "\n",
    "- $\\displaystyle \\eta^{|} = \\frac {\\eta}{\\sqrt{Sdw + \\epsilon}}$\n",
    "\n",
    "- $\\ Vdw_{t} = \\beta * Vdw_{t-1} + (1 - \\beta) * (\\displaystyle \\frac {\\partial Loss}{\\partial w_{t-1}}) $\n",
    "- $\\ Vdb_{t} = \\beta * Vdb_{t-1} + (1 - \\beta) * (\\displaystyle \\frac {\\partial Loss}{\\partial b_{t-1}}) $\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
