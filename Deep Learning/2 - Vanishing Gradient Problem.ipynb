{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing Gradient Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most asked Interview Question**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Simple Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider a Neural Network, having only one feature (x), and three hidden neurons, and one output neuron ($\\displaystyle \\hat{y} $).\n",
    "\n",
    "Now, as per the previous learning we had, weight $\\ w_1 $ and bias $\\ b_1 $ will be applied along with input $\\ x_1 $, and consequently, $\\ w_2 $ and $\\ b_2 $ will be applied for the second node, so on and so forth until the last neuron. \n",
    "\n",
    "Assume the outputs of each nodes is represented as $\\ O_{11}, O_{21}, O_{31}, O_{41} and O_{51} $.\n",
    "\n",
    "We have used the Sigmoid Activation Function ($\\displaystyle z = \\frac {1}{1 + e^{-x}} $) here for the hidden neurons, and the Loss function used is Mean Squared Error $\\displaystyle 1/2 * (y - \\hat{y})^2 $ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Propogation: Updation of Weights and Chain rule of derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the below mentioned formula to update all the four weights in our Neural Network. Let us first calculate the new weight of w1.\n",
    "\n",
    "$\\displaystyle w_{1 new} = w_{1 old} - \\eta * (\\displaystyle \\frac {\\partial Loss}{\\partial w_{1 old}}) $\n",
    "\n",
    "Using chain rule of derivatives formula to calculate the derivative:\n",
    "\n",
    "$ \\displaystyle \\frac {\\partial Loss}{\\partial w_{1 old}}  = \n",
    "\\displaystyle \\frac {\\partial Loss}{\\partial {O_{51}}} * \n",
    "\\displaystyle \\frac {\\partial O_{51}}{\\partial O_{41}} *\n",
    "\\displaystyle \\frac {\\partial O_{41}}{\\partial O_{31}} * \n",
    "\\displaystyle \\frac {\\partial O_{31}}{\\partial O_{21}} * \n",
    "\\displaystyle \\frac {\\partial O_{21}}{\\partial O_{11}} * \n",
    "\\displaystyle \\frac {\\partial O_{11}}{\\partial w_{1 old}} $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula for Sigmoid Activation Function $\\displaystyle z = \\frac {1}{1 + e^{-x}} $\n",
    "\n",
    "The value of z lies between 0 and 1. If the value of sigmoid activation function is >0.5, then the O/P will be 1, else if the value is <0.5 then the final output will be 0. \n",
    "\n",
    "But here since we need the derivation of Sigmoid activation function, the value lies between $\\ 0 \\leq \\displaystyle \\sigma(z) \\leq 0.25 $. \n",
    "\n",
    "Then the value of our derivative will be very less as the max value of derivative of sigmoid activation function is 0.25. \n",
    "Below are some example values where we have considered:\n",
    "\n",
    "$ \\displaystyle \\frac {\\partial Loss}{\\partial w_{1 old}}  =  0.25 * 0.15 * 0.10 * 0.05 * 0.02 $ ~ Smaller Value\n",
    "\n",
    "Hence, if we use the same in weights updation formula, then there is a high chance that New weight will be same as the old weight since, the derivative value is very less and negligible. \n",
    "\n",
    "$\\displaystyle w_{new} = w_{old} - \\eta (small value) $ and $\\ \\eta $ will also be less.\n",
    "\n",
    "So finally, if we see the value of new weight will be approximately equal to the old weight or else it will be the same as well.  \n",
    "\n",
    "$\\displaystyle w_{new} \\approx w_{old} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Observation and Definition of Vanishing Gradient Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, **during the back propogation when Sigmoid Activation function is used the weights are not getting updated because of its drawback of less value of derivative.**\n",
    "\n",
    "This problem is called as Vanishing Gradient Problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving this problem by using Different Activation Functions\n",
    "\n",
    "The Vanishing Gradient problem can be solved by using other activation functions apart from the Sigmoid activation function. Researchers most often prefer ReLU (Rectified Linear Unit) Activation function to solve this problem.\n",
    "\n",
    "Because of this occurence of this problem, we were not able to build a Deep Neural Network since the neural network will not be trained properly."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
