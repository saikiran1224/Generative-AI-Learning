{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function and types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation Function is a function where in which used in the Hidden layer neurons which will basically says how much the neuron should get activated or deactivated. Lets explore the various type of activation function which are most used: \n",
    "\n",
    "1. Sigmoid \n",
    "\n",
    "2. Tanhx \n",
    "3. ReLU (Rectified Linear Unit)\n",
    "4. Leaky ReLU\n",
    "5. Pre ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is defined as $\\displaystyle z = \\sigma(x) = \\frac {1}{1 + e^{-x}} $. \n",
    "\n",
    "Value of z lies between 0 and 1. The derivative of Sigmoid function lies between 0 and 0.25, due to which Vanishing Gradient problem exists. \n",
    "\n",
    "**Advantages:**\n",
    "- Smooth Gradient\n",
    "\n",
    "- Output values bound between 0 and 1, normalizing the values.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "- Prone to Vanishing gradient/Gradient Vanishing\n",
    "\n",
    "- Function output is Zero centered\n",
    "- Calculation of exponential values increases the time complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tanhx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is defined as $\\displaystyle z = \\frac {e^x - e^{-x}}{e^x + e^{-x}} $\n",
    "\n",
    "Value of tanhx lies between -1 to 1. The derivative of tanhx lies between 0 to 1. \n",
    "\n",
    "Unlike Sigmoid function, tanhx is a Zero Centered Curve (it means the curve passes through 0 when plotted in the graph and lies between -1 to 1). \n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Zero-centered curve\n",
    "\n",
    "- Output values bound between -1 to 1. \n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "- Still prone to Vanishing gradient \n",
    "\n",
    "- More time complexity as exponential operations are included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ReLU (Rectified Linear Unit) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most popular activation function which is used by the researchers. Denoted by \n",
    "\n",
    "$\\ ReLU = max(0, x) $ where $\\ x $ is the Input\n",
    "\n",
    "The simple definition of ReLU states two main things: \n",
    "- If x < 0 or x = 0, the final value of ReLU will be 0. \n",
    "- If x > 0, then final value of ReLU will be x. \n",
    "\n",
    "The derivative of ReLU activation function is either 0 or 1.\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- ReLU is much more quicker than other activation functions since its just a $\\ max() $ function. \n",
    "\n",
    "- Solves the Vanishing Gradient problem.\n",
    "\n",
    "**Disadvantage:**\n",
    "\n",
    "- Since the value is either 0 or 1 (derivative), consider if the value is 0 then the value of derivative in weights updation formula will be 0 making old weight again equal to the new weight. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
