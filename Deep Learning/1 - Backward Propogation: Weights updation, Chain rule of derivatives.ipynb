{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Propogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward propogation is defined as the backward process of Neural network computation. In the backward propogation, three main steps will happen: \n",
    "\n",
    "1. Calculation of Loss Function\n",
    "\n",
    "2. Updation of weights \n",
    "3. Chain Rule of Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Calculation of Loss function\n",
    "\n",
    "There are many types of Loss function, but most often we use the simple computatin of difference between the $\\ \\hat{y}$ and the actual output y. \n",
    "\n",
    "We can also prefer using the Mean Squared Error (MSE) which is calculated by squaring the distance between the predicted value and actual value, and dividing by 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Updation of Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main objective of updating weights while traversing backward is because, to minimize the value of loss function and make it near to 0. \n",
    "\n",
    "We need to use the below formula to update the weights. \n",
    "\n",
    "$\\displaystyle w_{new} = w_{old} - \\eta * (\\displaystyle \\frac {\\partial Loss}{\\partial w_{old}}) $\n",
    "\n",
    "where \n",
    "\n",
    "- $\\eta $ is the Learning Rate (often Lower values - 0.5, 1.0, etc. as required)\n",
    "\n",
    "- $\\displaystyle \\frac {\\partial Loss}{\\partial w_{old}} $ is the derivative of the Loss function w.r.t old weight\n",
    "\n",
    "\n",
    "Remember, in Simple Linear Regression algorithm we have plotted a graph Loss function over the weight, where we will get a curve looking like \"Gradient Descent\". We will traverse the points until we reach the Global minima which will be our Best fit line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative of Loss function is in -ve slope\n",
    "\n",
    "f the value of $\\displaystyle \\frac {\\partial Loss}{\\partial w_{old}} $ lies in the -ve slope (Left side of the gradient descent) then we need to **increase** the weight value towards right, then it will reach to the point of Global minima. \n",
    "\n",
    "Hence, then our $\\displaystyle \\frac {\\partial Loss}{\\partial w_{old}} $ will be Negative (<0)\n",
    "\n",
    "$\\ w_{new} = w_{old} - \\eta (-ve) $\n",
    "\n",
    "$\\ w_{new} = w_{old} + \\eta (+ve) $\n",
    "\n",
    "Hence, we can assume that $\\ w_{new} >> w_{old} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative of Loss function is in +ve slope\n",
    "\n",
    "f the value of $\\displaystyle \\frac {\\partial Loss}{\\partial w_{old}} $ lies in the +ve slope (Right side of the gradient descent) then we need to **decrease** the weight value towards left, then it will reach to the point of Global minima. \n",
    "\n",
    "Hence, then our $\\displaystyle \\frac {\\partial Loss}{\\partial w_{old}} $ will be Positive (>0)\n",
    "\n",
    "$\\ w_{new} = w_{old} - \\eta (+ve) $\n",
    "\n",
    "Hence, we can assume that $\\ w_{new} << w_{old} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance of Learning Rate ($\\displaystyle \\eta $) in formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some of the cases, once we have a point plotted in the curve which is not global minima and we are trying to plot the gradient descent curve but the new points are plotting somewhere else in the curve which is not leading to global minima. \n",
    "\n",
    "During that scenario, Learning rate helps us to achieve the point in form of gradient descent curve by adjusting the value as required. Most often, researches use the learning rate as 0.5 or 1.0, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chain Rule of Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above formula for calculating the new weights, we need to calculate the derivative of Loss function with respect to old weight. But, we cannot calculate directly since both the values are unknown. \n",
    "\n",
    "To solve this, we will use the Chain Rule of Derivatives, where we will choose the path backwards from the required weight. We will need the help of Output of each nodes, and the path traversing towards the desired weight which need to be updated.\n",
    "\n",
    "For example, Consider the perceptron, where $\\ w_4 $ will be applied after the computation of Hidden neuron, then the Final output of the O/P Neuron is denoted as $\\ O_2 $ here if we need to calculate the weight 4. Then the formula will be \n",
    "\n",
    "$\\displaystyle w_{4 new} = w_{4 old} - \\eta * (\\displaystyle \\frac {\\partial Loss}{\\partial w_{4 old}}) $\n",
    "\n",
    "then the value of $ \\displaystyle \\frac {\\partial Loss}{\\partial w_{4 old}} $\n",
    "\n",
    "$ \\displaystyle \\frac {\\partial Loss}{\\partial w_{4 old}} $ = $ \\displaystyle \\frac {\\partial Loss}{\\partial {O_2}} $  * $ \\displaystyle \\frac {\\partial O_2}{\\partial w_{4 old}} $\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
