{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Basic Chatbot using LangGraph (Graph API)\n",
    "\n",
    "Creating a Simple Chatbot where it takes the user prompt as an Input and provides the completion as an output using Graph API.\n",
    "\n",
    "**StateGraph:**\n",
    "\n",
    "\"START\" -> Node[Prompt passed to LLM] -> \"END\"\n",
    "\n",
    "Here, we have\n",
    "- 2 Edges (START to Node, Node to END)\n",
    "\n",
    "- 1 Node (Prompt passed to LLM)\n",
    "- State contains `messages` variable which gets appended everytime with the LLM response generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing LangGraph Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# LangGraph \n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages # It is a Reducer, its main role is to append new messages to the variable which its defined.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating State and building StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    \"\"\"\n",
    "    Messages have the type \"list\". The `add_messages` function in the annotation\n",
    "    defines how this state key should be updated. (In the current case, it appends messages to the list, rather than overwriting it.)\n",
    "    \"\"\"\n",
    "    messages: Annotated[list, add_messages] # Annotated is used to add metadata to the type, in this case, it specifies that the `messages` list should be updated using the `add_messages` reducer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x10a106000>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder = StateGraph(State)  # Create a StateGraph instance with the defined State type.\n",
    "\n",
    "graph_builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x10df7d550>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x10e100830>, model_name='llama3-8b-8192', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq # Type 1 Initialization \n",
    "from langchain.chat_models import init_chat_model # Type 2 Initialization - can be used with any chat model, not just Groq.\n",
    "\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\")\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Implementation for Chat Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatBot(state: State): # Inheriting the State, since the state is passed to the function, it can be used to access and modify the state variables.\n",
    "    return {\"messages\" : [llm.invoke(state[\"messages\"])]}  # The function takes the current state, invokes the LLM with the messages, and returns the updated messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
