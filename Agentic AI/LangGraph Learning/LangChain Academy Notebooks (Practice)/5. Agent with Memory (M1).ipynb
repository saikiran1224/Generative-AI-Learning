{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent with Memory\n",
    "\n",
    "The intution behind a simple agent architecture is **ReAct**. It does the following operations: \n",
    "- `act`: Performs the relevant tool call \n",
    "- `observe`: Post tool executed successfully, it returns back the response to the LLM\n",
    "- `reason`: LLM goes through the tool output, and reasons whether it needs to perform another tool call or generates its final response. \n",
    "\n",
    "**FYI** - For every graph execution when we call the `graph.invoke(dict)` function, the state is transient (temporary) and there will be no persistence. Hence, if we run the graph next time it doesn't have the previous interaction memory stored.\n",
    "\n",
    "To solve this problem, Langgraph has in-built **Memory Saver**, which saves every graph execution details like ids, state data, etc. in form of checkpoints. All these checkpoints are collectively stored in `threads`. \n",
    "\n",
    "Each thread has an `id` assigned, and in-short thread is the collection of checkpoints.\n",
    "\n",
    "For example, in our case:\n",
    "\n",
    "1. **First graph execution** - Graph runs \"add 2 and 3\" then it returns 5 with one tool call.\n",
    "\n",
    "2. **Second graph execution** - Graph runs \"multiply that with 5\" now it might hallucinate it doesn't know what is \"that\" referring to as it doesn't have previous grapg execution memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Agent\n",
    " \n",
    "We will be re-using the code written for developing Router, and as learnt about the \"Agent\", once the router routes the control to the tool node, the output of the tool node will be sent back to the tool_calling_llm, where the model observes and reasons whether it needs to do another tool call or not required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x104521910>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import START, END, StateGraph, MessagesState\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "from typing import TypedDict\n",
    "from langchain_core.messages import AIMessage, HumanMessage, AnyMessage, SystemMessage\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Define the LLM model \n",
    "llm = ChatGroq(model=\"qwen/qwen3-32b\")\n",
    "\n",
    "# Define the Messages State \n",
    "class MessagesState(MessagesState):\n",
    "    pass # add any other variables you need here\n",
    "\n",
    "# Defining tools \n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "# This will be a tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"Divide a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a / b\n",
    "\n",
    "tools = [multiply, add, divide]\n",
    "\n",
    "# Binding tools to the LLM\n",
    "llm_with_tools = llm.bind_tools(tools, parallel_tool_calls=False)\n",
    "\n",
    "# Define tool_calling_llm node \n",
    "def tool_calling_llm(state: MessagesState):\n",
    "    systemMessage = SystemMessage(content=\"You are a helpful assistant that can perform arithmetic operations.\")\n",
    "    return {\"messages\" : [llm_with_tools.invoke([systemMessage] + state[\"messages\"])]} \n",
    "\n",
    "# Define the graph\n",
    "builder = StateGraph(MessagesState) \n",
    "\n",
    "# adding nodes \n",
    "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# adding edges\n",
    "builder.add_edge(START, \"tool_calling_llm\")\n",
    "builder.add_conditional_edges(\"tool_calling_llm\", tools_condition) # This will call the tools if the LLM asks for it.\n",
    "builder.add_edge(\"tools\", \"tool_calling_llm\") # This will return to the LLM after the tool is called as per ReAct pattern. (Simple Agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Memory Saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver \n",
    "memory = MemorySaver() \n",
    "\n",
    "# Building and compiling graph with MemorySaver\n",
    "react_graph = builder.compile(checkpointer=memory)\n",
    "\n",
    "# Creating a thread which stores all the checkpoints inside\n",
    "config = {\"configurable\" : {\"thread_id\": \"cde\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoking the graph with config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Add 3 and 4\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  add (j8t7qm1ww)\n",
      " Call ID: j8t7qm1ww\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 4\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: add\n",
      "\n",
      "7\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The sum of 3 and 4 is $\\boxed{7}$.\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"Add 3 and 4\")]\n",
    "response = react_graph.invoke({\"messages\": messages}, config=config) # Adding the config\n",
    "\n",
    "for m in response[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Add 3 and 4\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  add (j8t7qm1ww)\n",
      " Call ID: j8t7qm1ww\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 4\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: add\n",
      "\n",
      "7\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The sum of 3 and 4 is $\\boxed{7}$.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Multiply that with 10\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (18wrrh5wa)\n",
      " Call ID: 18wrrh5wa\n",
      "  Args:\n",
      "    a: 7\n",
      "    b: 10\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "70\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The product of 7 and 10 is $\\boxed{70}$.\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"Multiply that with 10\")]\n",
    "response = react_graph.invoke({\"messages\": messages}, config=config) # Adding the config\n",
    "\n",
    "for m in response[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
