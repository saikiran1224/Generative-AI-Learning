{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoGen - AgentChat \n",
    "\n",
    "Using AgentChat, we can quickly build agentic applications using preset agents.\n",
    "\n",
    "In this notebook, let us start buildiong a single agent which uses tools. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os \n",
    "\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation of `agentchat` and `ext[openai]` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autogen-agentchat\n",
      "  Downloading autogen_agentchat-0.7.4-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting autogen-core==0.7.4 (from autogen-agentchat)\n",
      "  Downloading autogen_core-0.7.4-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: jsonref~=1.1.0 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from autogen-core==0.7.4->autogen-agentchat) (1.1.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.34.1 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from autogen-core==0.7.4->autogen-agentchat) (1.36.0)\n",
      "Requirement already satisfied: pillow>=11.0.0 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from autogen-core==0.7.4->autogen-agentchat) (11.3.0)\n",
      "Requirement already satisfied: protobuf~=5.29.3 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from autogen-core==0.7.4->autogen-agentchat) (5.29.5)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.10.0 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from autogen-core==0.7.4->autogen-agentchat) (2.11.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from autogen-core==0.7.4->autogen-agentchat) (4.14.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.7.4->autogen-agentchat) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.7.4->autogen-agentchat) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.7.4->autogen-agentchat) (0.4.1)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from opentelemetry-api>=1.34.1->autogen-core==0.7.4->autogen-agentchat) (8.6.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.34.1->autogen-core==0.7.4->autogen-agentchat) (3.21.0)\n",
      "Downloading autogen_agentchat-0.7.4-py3-none-any.whl (119 kB)\n",
      "Downloading autogen_core-0.7.4-py3-none-any.whl (101 kB)\n",
      "Installing collected packages: autogen-core, autogen-agentchat\n",
      "\u001b[2K  Attempting uninstall: autogen-core\n",
      "\u001b[2K    Found existing installation: autogen-core 0.6.4\n",
      "\u001b[2K    Uninstalling autogen-core-0.6.4:\n",
      "\u001b[2K      Successfully uninstalled autogen-core-0.6.4\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [autogen-agentchat]\n",
      "\u001b[1A\u001b[2KSuccessfully installed autogen-agentchat-0.7.4 autogen-core-0.7.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U \"autogen-agentchat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autogen-ext[openai]\n",
      "  Downloading autogen_ext-0.7.4-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: autogen-core==0.7.4 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from autogen-ext[openai]) (0.7.4)\n",
      "Requirement already satisfied: aiofiles in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from autogen-ext[openai]) (24.1.0)\n",
      "Requirement already satisfied: openai>=1.93 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from autogen-ext[openai]) (1.99.9)\n",
      "Requirement already satisfied: tiktoken>=0.8.0 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from autogen-ext[openai]) (0.9.0)\n",
      "Requirement already satisfied: jsonref~=1.1.0 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from autogen-core==0.7.4->autogen-ext[openai]) (1.1.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.34.1 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from autogen-core==0.7.4->autogen-ext[openai]) (1.36.0)\n",
      "Requirement already satisfied: pillow>=11.0.0 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from autogen-core==0.7.4->autogen-ext[openai]) (11.3.0)\n",
      "Requirement already satisfied: protobuf~=5.29.3 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from autogen-core==0.7.4->autogen-ext[openai]) (5.29.5)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.10.0 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from autogen-core==0.7.4->autogen-ext[openai]) (2.11.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from autogen-core==0.7.4->autogen-ext[openai]) (4.14.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.7.4->autogen-ext[openai]) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.7.4->autogen-ext[openai]) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.7.4->autogen-ext[openai]) (0.4.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from openai>=1.93->autogen-ext[openai]) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from openai>=1.93->autogen-ext[openai]) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from openai>=1.93->autogen-ext[openai]) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from openai>=1.93->autogen-ext[openai]) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from openai>=1.93->autogen-ext[openai]) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from openai>=1.93->autogen-ext[openai]) (4.67.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai>=1.93->autogen-ext[openai]) (3.7)\n",
      "Requirement already satisfied: certifi in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai>=1.93->autogen-ext[openai]) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai>=1.93->autogen-ext[openai]) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.93->autogen-ext[openai]) (0.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from opentelemetry-api>=1.34.1->autogen-core==0.7.4->autogen-ext[openai]) (8.6.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.34.1->autogen-core==0.7.4->autogen-ext[openai]) (3.21.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from tiktoken>=0.8.0->autogen-ext[openai]) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from tiktoken>=0.8.0->autogen-ext[openai]) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken>=0.8.0->autogen-ext[openai]) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/saikiran/miniconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken>=0.8.0->autogen-ext[openai]) (2.3.0)\n",
      "Downloading autogen_ext-0.7.4-py3-none-any.whl (328 kB)\n",
      "Installing collected packages: autogen-ext\n",
      "Successfully installed autogen-ext-0.7.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"autogen-ext[openai]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Gemini LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - [{'error': {'code': 404, 'message': 'models/gemini-1.5-flash-8b is not found for API version v1main, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Defining the model_client with Gemini 1.5 Flash 8B\u001b[39;00m\n\u001b[32m      5\u001b[39m model_client = OpenAIChatCompletionClient(\n\u001b[32m      6\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mgemini-1.5-flash-8b\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m     api_key=os.getenv(\u001b[33m\"\u001b[39m\u001b[33mGOOGLE_API_KEY\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      8\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m model_client.create([UserMessage(content=\u001b[33m\"\u001b[39m\u001b[33mWhat is the capital of India?\u001b[39m\u001b[33m\"\u001b[39m, source=\u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m)])\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py:691\u001b[39m, in \u001b[36mBaseOpenAIChatCompletionClient.create\u001b[39m\u001b[34m(self, messages, tools, tool_choice, json_output, extra_create_args, cancellation_token)\u001b[39m\n\u001b[32m    689\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    690\u001b[39m     cancellation_token.link_future(future)\n\u001b[32m--> \u001b[39m\u001b[32m691\u001b[39m result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[32m    692\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m create_params.response_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    693\u001b[39m     result = cast(ParsedChatCompletion[Any], result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:2589\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2543\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   2544\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mcreate\u001b[39m(\n\u001b[32m   2545\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2586\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   2587\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m   2588\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2589\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2590\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2591\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2592\u001b[39m             {\n\u001b[32m   2593\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2594\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2595\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2596\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2597\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2598\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2599\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2600\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2601\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2602\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2603\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2604\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2605\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2606\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2607\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2608\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2609\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   2610\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2611\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2612\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   2613\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2614\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2615\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2616\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2617\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2618\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2619\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2620\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2621\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2622\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2623\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2624\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2625\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: verbosity,\n\u001b[32m   2626\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2627\u001b[39m             },\n\u001b[32m   2628\u001b[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2629\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2630\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2631\u001b[39m         ),\n\u001b[32m   2632\u001b[39m         options=make_request_options(\n\u001b[32m   2633\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2634\u001b[39m         ),\n\u001b[32m   2635\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   2636\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2637\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2638\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/openai/_base_client.py:1794\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1780\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mpost\u001b[39m(\n\u001b[32m   1781\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1782\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1789\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1790\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1791\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1792\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1793\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/openai/_base_client.py:1594\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1591\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1593\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1594\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1596\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1598\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNotFoundError\u001b[39m: Error code: 404 - [{'error': {'code': 404, 'message': 'models/gemini-1.5-flash-8b is not found for API version v1main, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}]"
     ]
    }
   ],
   "source": [
    "from autogen_core.models import UserMessage\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "# Defining the model_client with Gemini 1.5 Flash 8B\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gemini-1.5-flash-8b\",\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    ")\n",
    "\n",
    "response = await model_client.create([UserMessage(content=\"What is the capital of India?\", source=\"user\")])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Fake function call for Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_weather(city: str) -> str:\n",
    "    \"\"\"Get the weather for a given city.\"\"\"\n",
    "    return f\"The weather in {city} is 73 degrees and Sunny.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent # Importing Assistant agent \n",
    "from autogen_agentchat.ui import Console # Importing Console UI\n",
    "\n",
    "weather_agent = AssistantAgent(\n",
    "    model_client=model_client,\n",
    "    name=\"weather_agent\",\n",
    "    tools=[get_weather],  # Registering the get_weather tool\n",
    "    system_message=\"You are a helpful weather assistant. You can provide the current weather for any city using the get_weather tool.\",\n",
    "    reflect_on_tool_use=True,  # Enabling reflection on tool use\n",
    "    model_client_stream=True,  # Enabling streaming responses\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n",
      "What is the weather in San Francisco?\n",
      "---------- ToolCallRequestEvent (weather_agent) ----------\n",
      "[FunctionCall(id='', arguments='{\"city\":\"San Francisco\"}', name='get_weather')]\n",
      "---------- ToolCallExecutionEvent (weather_agent) ----------\n",
      "[FunctionExecutionResult(content='The weather in San Francisco is 73 degrees and Sunny.', name='get_weather', call_id='', is_error=False)]\n",
      "---------- ModelClientStreamingChunkEvent (weather_agent) ----------\n",
      "The weather in San Francisco is 73 degrees and sunny.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async def main() -> None: \n",
    "    # Running the agent in a console UI\n",
    "    await Console(weather_agent.run_stream(task=\"What is the weather in San Francisco?\"))\n",
    "    await model_client.close()\n",
    "\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
