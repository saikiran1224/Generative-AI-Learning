{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression (Binary Classification)\n",
    "\n",
    "This is part of Supervised Learning > Classification problem. \n",
    "\n",
    "The output of this model will be fixed and here we will be dealing with two values as its a binary classification. This algorithm works well with Multi-classification as well. \n",
    "\n",
    "Eg. No. of study hours vs Pass/Fail \n",
    "\n",
    "Likewise Linear Regression, we can plot the data points over a graph and find the best fit line for the above example. But it works fine for all the given data, but the best fit line fails when there is a **Outlier** or when a new **input/test data** is passed to the model.\n",
    "\n",
    "But there were some drawbacks identified or how linear regression is not suitable here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawbacks if we use Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon using the best fit line of linear regression, it will be a line passing through origin and it will pass to the points as per the given training data. But there is a catch here, once if we give a new input to the model which is far to the best fit line then automatically due to the long distance a new best fit line will be formed which affects the earlier values. \n",
    "\n",
    "It means, the new best fit line which is formed will show the previous values where the output is True as False, which is completely not accepted. To overcome this drawback with the Linear regression, we need to squash (make the line straight) so the output will be as expected. \n",
    "\n",
    "For squashing the line, we will be using the help of Sigmoid function which will be applied on the already present cost function of linear regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function helps us to squash the best fit line. Since the main objective of Logistic regression is the output values should be between 0 and 1. So lets derive the same.\n",
    "\n",
    "Consider we know the equation of the best fit line:\n",
    "\n",
    "h $\\theta(x)$ = $\\theta_0$ + $\\theta_1$ * (x) where h $\\theta(x)$ is the predicted value. \n",
    "\n",
    "So let z = $\\theta_0$ + $\\theta_1$ * (x), then we can derive the above equation as follows: \n",
    "\n",
    "h $\\theta(x)$ = g(z) \n",
    "\n",
    "h $\\theta(x)$ = 1 / (1 + $\\exp^{-z}$) \n",
    "\n",
    "then h $\\theta(x)$ = 1 / (1 + $\\exp^{-(theta_0 + theta_1 * (x)}) $\n",
    "\n",
    "So if we draw a graph for the above function then we can observe that g(z) >/ 0.5 when z >/ 0. (Note: >/ means greater than or equal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deriving Cost function of Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets consider the training data as (x1, y1), (x2, y2), (x3, y3), ... (xn, yn). Here remember that y will lie between 0 and 1. \n",
    "\n",
    "Remember the cost function of linear regression, \n",
    "\n",
    "J($\\theta_0$, $\\theta_1$) = 1 / 2m * $\\sum_1^m$ (h $\\theta(x^i)$ - y^i) ^ 2\n",
    "\n",
    "here we will replace the h $\\theta(x^i)$ with the sigmoid function which we derived above, then the final function will be the h$\\theta(x^i)$ replaced by the sigmoid function 1 / (1 + $\\exp^{-(theta_0 + theta_1 * (x))}) $.\n",
    "\n",
    "But, the function which was derived just now is a **Non-convex function** which is not suitable to find the global minima. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between Convex and Non-Convex functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convex Function** - This a function in which its in the shape of Gradient Descent and there will be a point where there is Global minima. Remember, in Linear regression the cost function graph will look like the same which helps us to easily identify the best fit line. \n",
    "\n",
    "**Non-Convex Function** - This is a function in which the graph is irregularized and it has multiple local minimas making the model to not work properly and also not able to find the best fit line at the point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Cost Function for Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, since the above derived function is a Non-Convex function we will not be proceeding with this ahead. \n",
    "\n",
    "But as per the researcher, the final **Cost Function of Logistic Regression** is mentioned below considering $\\theta_0$ = 0.\n",
    "\n",
    "J($\\theta_1$) = -$\\log( h \\theta(x^i)$) when y = 1\n",
    "\n",
    "J($\\theta_1$) = -log(1 - h $\\theta(x)$) when y = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
