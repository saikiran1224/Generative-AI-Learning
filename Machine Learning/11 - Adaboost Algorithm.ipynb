{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost uses the Boosting Technique, where models (weak learners) are joined sequentially to get a strong output. \n",
    "\n",
    "In Adaboost as well, we will use the help of Decision Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding algorithm with Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider a classification dataset consisting of four features f1, f2, f3 and f4 and target variable as Yes/No having 7 records inside.\n",
    "\n",
    "- We first need to calculate the weight of each data point and assign to each row. Weight is something which adds value to the particular row, which we will understand more about it better as we go forward. Remember the total weight of all the data points should be equal to 1.\n",
    "\n",
    "- Here, since we have seven rows we will assign weight for each row as 1/7. So, if we do the sum of all the rows the output will be equal to 1.\n",
    "\n",
    "- Its time to use decision tree. Among all the features (f1, f2, f3, f4) the feature having high information gain and entropy will be considered first and splitted ONLY single/one level. \n",
    "  \n",
    "  Single level decision tree is referred as **Stump**. Its also called as the Weak learner. \n",
    "\n",
    "- Now the above stump will be trained with the training data which we just prepared. Once its done, we will test with some input value and will identify for which rows the prediction is wrong. In short, we need to calculate the **Total Error**. Our stump has predicted wrong for only one row.\n",
    "\n",
    "  $\\ Total Error = \\dfrac{Number  of Wrongly predicted values}{Total Number of values} = 1 / 7$\n",
    "\n",
    "- Next we need to calculate the Performance of our Stump using the below formula\n",
    "\n",
    "  $\\ Performance (stump) = \\dfrac{1}{2} * log_e (\\dfrac{1 - TE}{TE})$ = 0.895\n",
    "  \n",
    "- Now, We need to update the sample weights. Remember, for correct records, weight should be lowered and for wrong records, the weight should be increased. This helps the next weak learner to focus more on the wrong records than the correct records. We will use help of below formula to calculate the weights of correct and wrong records. \n",
    "\n",
    "  $\\ Weight (correct records) = weight * e^-(P_s) = 1/7 * e ^ (-0.895) = 0.05 $\n",
    "  \n",
    "  $\\ Weight (wrong records) = weight * e^(P_s) = 1/7 * e ^ (0.895) = 0.349 $\n",
    "\n",
    "- The new calculated weights be updated in our dataset. But if we sum up all the weights it will not lead to 1. Hence, we need to normalize the values further to make it reach to 1. Once done the updated weigths will be 0.07 for correct records and 0.537 for incorrect records. \n",
    "\n",
    "- Now, we need to divide the weights into buckets. The bucket range starts from 0 and ends for the last record at 1. Here, for the first row the bucket will be [0 - 0.07], similarly [ 0.07 - (0.07 + 0.07)] = [0.07 - 0.14], [0.14 - 0.21], etc. The bucket size will be more at the row where the wrong predicted record exists. It will look like [0.21 - 0.747]. \n",
    "\n",
    "- Among the whole dataset, the bucket size will be more here making it more highlighted than the other data points.\n",
    "\n",
    "- Now, the next stump will be created and the above data will be sent. Similarly the same process gets repeated and most cases there will be around 100 decision trees (stumps) gets created which will predict a Strong output."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
