{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xgboost Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xgboost is abbreviated as eXtreme Gradient Boosting. This algorithm is one of the boosting technique where it works on underlying decision trees. Its also a Black box model since we will not be analyze the decision trees using a pen and paper and manually to predict the output.\n",
    "\n",
    "Xgboost solves both regression and classification problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider that there is a weak learner or base model (decision tree) where it will only predict the output or probabilty as 0. 5 irrespective of the output given to the model. Any test data that we pass to the model, it will only give output as 0.5. \n",
    "\n",
    "Now, as we know in boosting technique we will arrange weak learners in a sequential manner, and the output will be passed from one weak learner to another making it finally as Strong learner and giving better output. Similary, here also using one decision tree or the weak learner above we need to combine multiple weak learners together sequentially. \n",
    "\n",
    "Let us understand the three major steps involved in the Xgboost algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps involved\n",
    "\n",
    "There are mainly three steps involved in this algorithm which applies the same for both Classification and regression problems. The following are the steps considered a dataset is given: \n",
    "\n",
    "1. Create a binary decision tree based on one feature.\n",
    "\n",
    "2. Calculate similarity weight at each level in the decision tree using the below formula: \n",
    " \n",
    "   $\\displaystyle Similarity weight$ = $\\displaystyle \\displaystyle \\frac {\\sum (Residual)^2} {\\sum (P_r (1 - P_r)) + \\lambda} $\n",
    "\n",
    "   where \n",
    "   - $\\ P_r$ is the probability of the base model or weak learner\n",
    "   - $\\ Residual$ is the difference between the $\\ P_r$ and the Output variable of each record.\n",
    "   - $\\displaystyle \\lambda$ is the hyperparameter. \n",
    "\n",
    "3. Calculate the Information gain. It can be calculated by simply addding the similarity of both the child nodes and subtracting it from similarity of Root node.\n",
    "\n",
    "   $\\displaystyle Information Gain = \\displaystyle Similarity(Child node 1) + Similarity(Child node 2) - Similarity(Root node)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Consider a dataset having two features (Salary, Credit Score) and one output variable (Approval - 0 or 1). \n",
    "\n",
    "| Salary | Credit Score | Approval | Residuals (Approval - $\\ P_r$)  |  \n",
    "| :----: | :----: | :----: | :----: | \n",
    "| <= 50 | Bad | 0 | 0 - 0.5 = -0.5 |\n",
    "| <= 50 | Good | 1 | 1 - 0.5 = 0.5 |\n",
    "| <= 50 | Good | 1 | 1 - 0.5 = 0.5 |\n",
    "| > 50 | Bad | 0 | 0 - 0.5 = -0.5 |\n",
    "| > 50 | Good | 1 | 1 - 0.5 = 0.5 |\n",
    "| > 50 | Neutral | 1  | 1 - 0.5 = 0.5 |\n",
    "| <= 50 | Neutral | 0 | 0 - 0.5 = -0.5 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Create a binary decision tree using the Salary feature\n",
    "\n",
    "Keeping the salary feature as the root node, as we infere from the dataset we will define two branches: $\\ <= 50$ and $\\ >50$. In the dataset we already calculated the residual for each record by subtracting the $\\ P_r$ from the Approval. \n",
    "\n",
    "Now, we will write the residual in form of list at each node. \n",
    "\n",
    "- Root node(all residuals will come): [-0.5, 0.5, 0.5, -0.5, 0.5, 0.5, -0.5]\n",
    "\n",
    "- Child node 1 (Residuals of Salary <=50): [-0.5, 0.5, 0.5, -0.5]\n",
    "- Child node 2 (Residuals of Salary >50): [-0.5, 0.5, 0.5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Calculate Similarity weight at each node\n",
    "\n",
    "Using the formula stated above, we will calculate the similarity weight. Consider $\\lambda$ value as 0.\n",
    "\n",
    "- Root node:\n",
    "   \n",
    "   Similarity weight(Root node) = $\\displaystyle \\frac {(-0.5 + 0.5 + 0.5 - 0.5 + 0.5 + 0.5 -0.5)^2}{(0.5(1-0.5)) + (0.5(1-0.5)) + (0.5(1-0.5)) + (0.5(1-0.5)) + (0.5(1-0.5)) + (0.5(1-0.5)) + (0.5(1-0.5)) + 0 }$\n",
    "\n",
    "   = 0.25 / 1.75 = **0.14**\n",
    "\n",
    "Similarly calculate the same for both the child nodes\n",
    "\n",
    "- Child node 1 (<=50)\n",
    "  \n",
    "  Similarity weight (Child node 1) = **0** \n",
    "\n",
    "- Child node 2 (>50)\n",
    "  \n",
    "  Similarity weight (Child node 2) = **0.33**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Calculate the Information gain\n",
    "\n",
    "Information Gain = 0 + 0.33 - 0.14 = **0.19**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Inference calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as we can see we got information gain of 0.19, as per the principle of decision tree the higher information gain feature will be selected and splitted further. Hence, we will follow the above process once again and now split the Credit feature further and calculate the information gain. \n",
    "\n",
    "Likewise we will get multiple binary decision trees named DT 1, DT 2, DT 3, ... DT n. So the final inference for any test record it can be calculated using the sigmoid function of sum of product of learning rate ($\\ alpha$ with the respective DT). \n",
    "\n",
    "Final Inference of any record = $\\displaystyle \\sigma [0 + \\alpha (DT_1) + \\alpha (DT_2) + \\alpha (DT_3) + \\alpha (DT_4) + ...  + \\alpha (DT_n)]$ = Strong learner\n",
    "\n",
    "where \n",
    "\n",
    "- $\\displaystyle \\sigma$ is the Sigmoid function\n",
    "- $\\displaystyle \\alpha$ is the Learning rate\n",
    "- $\\ DT_1$ is the Decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything will be the same as in terms of classification. We will not be considering the probability of the base model as its a Regression problem and the output is a continous variable. \n",
    "\n",
    "Instead of probability, we will use the help of mean functionality. While calculating the residuals, we will subtract the target data variable from the average of the target variables.\n",
    "\n",
    "For example, our dataset contains the salary as the target variable, first we will calculate the average of the salary column, and for residual calculation we will peform the following: Mean - Target variable(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also, there is a slight change in similarity weight. It will be calculated as follows: \n",
    "\n",
    "$\\displaystyle Similarity weight$ = $\\displaystyle \\displaystyle \\frac {\\sum (Residual)^2} {Number of residuals + \\lambda} $"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
