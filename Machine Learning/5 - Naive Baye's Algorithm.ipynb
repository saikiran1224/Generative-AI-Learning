{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Baye's Algorithm\n",
    "\n",
    "This comes under Supervised Learning > Classification\n",
    "\n",
    "Main crux behind this algorithm is the Baye's algorithm. Before that let's learn more about the Conditional Probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first understand the difference between Independent and dependent events. \n",
    "\n",
    "- **Independent events:** For example, consider a die has been rolled and the probability of occurence of 1, 2, 3, 4, 5, 6 are completely independent and occurence of one event does not affect the other. \n",
    "\n",
    "- **Dependent events:** For example, consider in a bag there are 2 red balls and 3 blue balls. In the first event the probability of picking up a red ball from the bag will be 2 / (2+3) = 2/5. In the second event, the probability of picking up a blue ball will be 3/4 since one ball is already picked up. So the second event is dependent on the occurence of the first event. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of Conditional Probability - Let us consider two events A and B, then conditional probability is defined as the probability of occurence of event A given B (or) probability of occurence of event A given B is already occcurred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically it is defined as $\\ P(A and B) = P(A) * P(B/A) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baye's Theorem Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know already that $\\ P(A and B) = P(A) * P(B/A) $, so\n",
    "\n",
    "$\\ P(A and B) = P(B and A) $\n",
    "\n",
    "$\\ P(A) * P(B/A) = P(B) * P(A/B) $\n",
    "\n",
    "$\\ P(B/A) = P(B) * P(A/B) / P(A)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it can be used as Classification algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider our dataset has four features x_1, x_2, x_3, x_4 and a target variable named y. \n",
    "\n",
    "So, considering the derived bayes theorem we can dervive one formula with the above dataset. \n",
    "\n",
    "$\\ P(y / x_1, x_2, x_3, x_4) = P(y) * P(x_1, x_2, x_3, x_4 / y) / P(x_1, x_2, x_3, x_4)$\n",
    "\n",
    "=> $\\ P(y) * P(x_1/y) * P(x_2/y) * P(x_3/y) * P(x_4/y) / P(x_1) * P(x_2) * P(x_3) * P(x_4)$\n",
    "\n",
    "The denominator can be ignored since it will be always constant, then the final value will be\n",
    "\n",
    "$\\ P(y / x_1, x_2, x_3, x_4) = P(y) * P(x_1/y) * P(x_2/y) * P(x_3/y) * P(x_4/y)$\n",
    "\n",
    "It can be further divided into two values, since its a classification algorithm the target variable consists of two values - Yes or No. So the above derived formula can be derived in such a way like: \n",
    "\n",
    "$\\ P(y = 'Yes' / x_1, x_2, x_3, x_4) = P(y) * P(x_1/y) * P(x_2/y) * P(x_3/y) * P(x_4/y)$\n",
    "\n",
    "$\\ P(y = 'No' / x_1, x_2, x_3, x_4) = P(y) * P(x_1/y) * P(x_2/y) * P(x_3/y) * P(x_4/y)$\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
