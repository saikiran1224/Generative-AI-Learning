{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier and Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawbacks of Decision Tree Alogorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before deep diving into Random Forest, let us have a look on the drawbacks or challenges of the Decision Tree algorithm: \n",
    "- Performs Overfitting because tree gets divided till the maximum depth (model works good for Training data showing Low bias and High variance)\n",
    "- Will not be able to handle outliers.\n",
    "\n",
    "Random Forest will help us to overcome all the above stated drawbacks and helps us to develop a generalized model (Low bias and Low Variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest uses the bagging technique which means we will be sampling the features and data points in our dataset and provide it to multiple models and train them seperately and parallely. \n",
    "\n",
    "Here, in **Random Forest all the models used will be Decision Trees** only. It will be Decision Tree 1, Decision Tree 2, etc. so on and so forth. There may be a chance that while sampling the data points, the features and data points may get repeated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Random Forest eliminates overfitting by the majority voting/boostrap aggregator, since we are considering the multiple outputs of the decision tree and finding the best output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the test input data is given to all the models, each model predicts their respective output. Among the predicted output, we will be considering the value which is repeated more times a.k.a majority. This is called as Majority voting or bootstrap aggregator. \n",
    "\n",
    "For example, consider there are four decision trees - D1, D2, D3, and D4. Once the test input is passed, the outputs predicted by each model are as follows: \n",
    "- D1: 0\n",
    "- D2: 1\n",
    "- D3: 0\n",
    "- D4: 0\n",
    "\n",
    "As per the majority voting/boostrap aggregator, since three DTs predicted output as 0, our final best output is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the classification problem, the process remains same till output prediction of individual decision trees. Since, its a regression problem the output of all the decision trees will be continous in nature. \n",
    "\n",
    "We need to calculate the average or mean of all the predicted values, which will be the Final Output. \n",
    "\n",
    "For example, consider the same that we have multiple decision trees and the outputs of each DTs are as follows: \n",
    "- D1: 23.4\n",
    "- D2: 22.0\n",
    "- D3: 21.6\n",
    "- D4: 20.0\n",
    "\n",
    "The Final output will be the average of all the above values which will be approximately 21.75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Points to be noted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Do we require Normalization or Standardization in Random Forest?**\n",
    "   \n",
    "   No, since decision Tree(s) predicts the values based on splits so further if the data split is done there will be of no use. But Normalization is highly required in K - Nearest Neighbous since we will be using the Euclidean or Manhattan distance to calculate the distances between the test point and the data point. To reduce the computational costs and increase the efficency, we need to use Standardization or Normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Does Random Forest is affected by Outliers?**\n",
    "   \n",
    "   No, considering in terms of regression we will average or mean the values of all the outputs predicted by decision trees eliminating the outliers or noisy data. It also splits the data based on the ranks instead of absolute values."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
