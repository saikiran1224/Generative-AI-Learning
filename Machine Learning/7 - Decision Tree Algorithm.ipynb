{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Algorithm for Classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree is like a visualized nested if condition. This algorithm works for both Supervised Machine Learning > Regression and Classification problems. \n",
    "\n",
    "Mostly all the researchers and in research papers the Overcast, Temp, Humidity dataset will be used. So lets re-use the same dataset again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dataset](https://i.sstatic.net/KYSy4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding and analyzing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the target variable we have around 9 Yes and 5 No. \n",
    "\n",
    "Now, lets select the first feature which is `Outlook` and break down it further in form of a decision tree.\n",
    "\n",
    "The first parent node, will be `Outlook` and inside outlook we have total three type of values which are Sunny, Overcast and Rain. So our child notes after outlook will be Sunny, Overcast and Rain. \n",
    "\n",
    "For each child feature let's name it as c1 - Sunny, c2 - Overcast and c3 - Rain.\n",
    "\n",
    "- c1 - Sunny, we have 2 Yes and 3 No => **Impure split**\n",
    "\n",
    "- c2 - Overcast, we have 4 Yes and 0 No => **Pure Split**\n",
    "\n",
    "- c3 - Rain, we have 3 Yes and 2 No => **Impure split**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Pure and Impure Split? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the feature is divided further into its sub-categories similar to the categories in the above data, we will calculate the Yes and No at each category. \n",
    "\n",
    "**Pure Split** - If the category has all Yes and zero No (or) zero Yes and all No then we DONT NEED to divide it further as we already have the answer. Here, c2 - Overcast is a Pure split.\n",
    "\n",
    "**Impure Split** - If the category has inequal number of Yes and No (or) equal number of Yes and No then we NEED to divide it further as we are unsure about the answer. Here, c1-Sunny and c3-Rain are the Impure splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### But here since the dataset is simple, by observing we are able to identify which is a Pure split and which is Impure split. In the large dataset case, its difficult to identify, for which we have two types of techniques. They are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniques to identify Pure or Impure split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Entropy H(s) - *using logarithm*\n",
    "\n",
    "Once the feature is divided into sub categories, we will be calculating the entropy represented by $\\ H(s)$ with the number of Yes/No at that point of category using the below formula:\n",
    "\n",
    "$\\ H(s) = - P_+ log_2 P_+ - P_- log_2 P_-$\n",
    "\n",
    "where \n",
    "$\\ P_+$ - Probability of Yes\n",
    "\n",
    "$\\ P_-$ - Probability of No\n",
    "\n",
    "Remember, the value of entropy lies between $\\ 0 < H(s) < 1 $\n",
    "\n",
    "Upon calculating if the entropy of the category is $\\ H(s) = 0$ then its a **Pure Split**, and if $\\ 0 < H(s) < 1$ then its a **Impure split**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Gini Impurity - *simple mathematics*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree classifier by default uses the Gini Impurity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the entropy, Gini Impurity is also used for the same purpose where it will indicate whether the node needs to be splitted further or not. The formula used will be different here. \n",
    "\n",
    "Gini Impurity = $\\ 1 - \\sum_1^m (P)^2 $ = $\\ 1 - (P_+^2 + P_-^2) $ \n",
    "\n",
    "The value of Gini Impurity lies between **0 < Gini Impurity < 0.5**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### If our dataset has multiple features, how do we know that from which feature we need to start dividing (or) which feature needs to be first selected to predict the output. To identify that, we have Information Gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Gain - $\\ Gain(S,f_1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree classifier will calculate the Information gain for all the feaures in the dataset, and the **feature having the highest Information gain value will be considered first to divide further**. \n",
    "\n",
    "In the similar fashion, we have considered the Outlook feature for division. To calculate the Information gain represented by $\\ Gain(S,f_1)$, we need to use the below formula: \n",
    "\n",
    "$\\ Gain(S,f_1) = H(S) - \\sum_v\\dfrac {|S_v|}{|S|} H(S_v) $\n",
    "\n",
    "where $\\ H(S)$ - Entropy of the root node\n",
    "\n",
    "$\\sum_v\\dfrac {|S_v|}{|S|} H(S_v)$ - Total Sum of the entropies of each category $\\ H(S_v)$, multiplied by division of Sum of Yes and No at each category ($\\ |Sv|$) divided by Sum of total Yes and No ($\\ |S|$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Gini Impurity is faster than Entropy since Entropy involves logarithmic operations wherease Gini Impurity requires simple mathematics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset having features as continuous variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the features of the dataset are continous variables, then the features will be arranged in the Ascending order and need to calculate the Information gain as usual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree for Regression problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the regression problem, the output variable is Continous. As mentioned earlier, we can use the decision tree for regression problem as well. The calculation and terminologies might differ when compared to the classifier model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will use the help of MSE or MAE instead of entropy to know whether the node is a pure split or impure split. \n",
    "\n",
    "If the MSE gets reduced, then its a signal means that we are reaching to the leaf node, until then we need to split the features. MSE is nothing but the cost function which we used in the Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly Decision trees takes the higher time complexity as it calculates the Entropy/Gini Impurity for the nodes and proceeds further. \n",
    "\n",
    "Decision trees does go to Overfitting. To overcome/solve this, we have two types of techniques: \n",
    "\n",
    "1. **Post-pruning:** While dividing the nodes, if one of the node has more number of Yes than No, then we will go ahead and consider the answer as Yes. \n",
    "\n",
    "2. **Pre-pruning:** We will use the help of some hyper parameters named max_depth, max_leaf which can be obtained from GridSearchCV library."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
