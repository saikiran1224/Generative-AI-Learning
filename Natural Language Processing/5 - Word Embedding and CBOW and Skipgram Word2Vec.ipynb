{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embeddings is a technique where in which it will converts the words in a document or corpus into vectors making it easy to train the model using Deep Learning and Machine Learning. \n",
    "\n",
    "Word Embedding can be further divided into two categories: \n",
    "\n",
    "1. Based on count or frequency:\n",
    "     - Bag of Words (BOW)\n",
    "     \n",
    "     - TFIDF\n",
    "     - One Hot Encoding\n",
    "\n",
    "2. Deep Learning trained model\n",
    "     - Word2Vec\n",
    "\n",
    "     - AvgWord2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawbacks of BOW and TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sparse Matrix (Whenever a new text input probably a test message is given, then the matrix will be almost 0s for all the features resulting in sparsity)\n",
    "\n",
    "2. Unable to find the semantic meaning (TFIDF treats the words independently not considering its surrounding context or relationship)\n",
    "\n",
    "To overcome the above two drawbacks, we will use the Word2Vec technique where it uses the neural network based technique which improves the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec \n",
    "\n",
    "Its a feature representation technique. Among all the words to vectors conversion technique, Word2Vec stands efficient as of today. Another flavour of Word2Vec is AvgWord2Vec which looks more efficient than the traditional Word2Vec algorithm.\n",
    "\n",
    "To clearly understand the working of Word2Vec, Artificial Neural Network is the pre-requisite. \n",
    "\n",
    "## Key points:\n",
    "\n",
    "When using Word2Vec technique, \n",
    " \n",
    "1. The vectors will be created in Fixed dimension or limited dimension.\n",
    "\n",
    "2. Sparsity will be reduced as the dimension is fixed already.\n",
    "3. Semantic meaning will be maintained as its using Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider from the given sentence, we have total five features: Boy, Girl, King, Queen and Apple termed as f1, f2, f3, ... f5.\n",
    "\n",
    "While working with Word2Vec, we can use it in two ways: One is we can use the pre-trained model by Google having 3 Million dimensions like {Gender, Royal, Age, Food, etc...}, and the other one where we can prepare our own model using the Word2Vec library. \n",
    "\n",
    "Here, let us use the Google's pre-trained model and pass on our features. Then for each feature we will get an individual 3 billion dimension vector. \n",
    "\n",
    "For example, the first feature Boy gets converted to {-1, 0.01, 0.03, .... 0.90}. The values like -1, 0.01 are calculated by the model how the current feature is correlative to the feature in the pre-trained model. Here, our feature is \"Boy\" so the vector value will be 0.98 where the corresponding feature is \"Gender\" in the pre-trained model.\n",
    "\n",
    "Likewise, for all the features vectors gets created. Now using these vectors we can do the comparisons. We can also provide a word and the pre-trained model gives the much related words based on the co-sime similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity and Cosine Distance\n",
    "\n",
    "This technique helps us know how two vectors are how much related to each other. Let us consider we have two vectors P1, and P2. To know the similarity between these two vectors, we will use the below formula:\n",
    "\n",
    "Cosine Distance = 1 - Cosine Similarity = 1 - $\\ cos \\theta$ where $\\ \\theta$ is the angle between the two vectors.\n",
    "\n",
    "0 (More Similar) < **Cosine Similarity** < 1 (Less Similar)\n",
    "\n",
    "0 (Less Similar) < **Cosine Distance** < 1 (More Similar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Word2Vec: \n",
    "\n",
    "1. Continous Bag of Words (CBOW)\n",
    "\n",
    "2. Skipgram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Continous Bag of Words (CBOW)\n",
    "\n",
    "In this technique, once the corpus is broken down into sentences we will apply the Continous Bag of words (CBOW) and convert each word into a vector.\n",
    "\n",
    "For example, lets consider a simple document - [Krish Channel is related to Data Science].\n",
    "\n",
    "#### Step 1: Specify the Window size\n",
    "\n",
    "It is like a hyperparameter. It is highly recommended to choose an Odd number. Once the window size is specified, we will create a table of Input and Output. \n",
    "\n",
    "Let us consider the window size = 5. First consider the 5 words from the start, then first 2 words and last 2 words will be our Input variables and the middle variable will be the output. \n",
    "\n",
    "[**Krish Channel** is **related to**] - O/P is \"is\". Similarly lets create the input and output table from this document.\n",
    "\n",
    "| Input variables | Output variable | \n",
    "| :---: | :---: |\n",
    "| Krish, Channel, related, to | is |\n",
    "| Channel, is, to, Data | related |\n",
    "| is, related, Data, Science | to |\n",
    "\n",
    "#### Step 2: Bag of Word/OHE representation\n",
    "\n",
    "Now, lets convert the above each word in the document in form of OHE or Bag of words representation. \n",
    "\n",
    "| Word | Vector | \n",
    "| :---: | :---: |\n",
    "| Krish | 1 0 0 0 0 0 0 |\n",
    "| Channel | 0 1 0 0 0 0 0 |\n",
    "| is | 0 0 1 0 0 0 0 |\n",
    "| related | 0 0 0 1 0 0 0 |\n",
    "| to | 0 0 0 0 1 0 0 |\n",
    "| Data | 0 0 0 0 0 1 0 |\n",
    "| Science | 0 0 0 0 0 0 1 |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Step 3: Building ANN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
