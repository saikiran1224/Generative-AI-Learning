{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before understanding about the attention model, lets understand about the drawbacks of the Encoders and Decoders. \n",
    "\n",
    "- Encoders and Decoders both work in a uni-directional way. \n",
    "\n",
    "- The context vector which is generated by the Encoder doesn't contain the whole context of the sentence in case if the sentence contains more words. \n",
    "- It will remember the context or memory of the sentences which are nearby only. \n",
    "- Fixed length context vector generation is eliminating the sentence importance leading in performance issues.\n",
    "\n",
    "Also if we discuss one more use case as in such, we have the below sentence and need to predict the word in the blank.  \n",
    "\n",
    "Krish likes to eat ______ in Bengaluru.\n",
    "\n",
    "In the above case, Encoder and Decoder may get failed because it has the context till 'likes to eat' but it doesn't have the context of 'in Bengaluru'. Before the prediction, our model should know the context of the forward words as well. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Attention Model is a mechanism that helps neural networks focus on the most relevant parts of an input sequence while making predictions. It dynamically assigns different importance (weights) to different words or tokens, improving tasks like translation, summarization, and sentiment analysis.\n",
    "\n",
    "Initially, the LSTMs in the Encoder - Decoders architecture are replaced by Bi-directional LSTM RNNs as it have the bi-directional data movement and preserve context of both forward and backward words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole architecture is modified as below: \n",
    "\n",
    "Encoder => Bi-directional LSTM RNN\n",
    "\n",
    "Decoder => LSTM RNN with minimal changes or no change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bi-directional LSTM RNN](https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs40537-022-00664-6/MediaObjects/40537_2022_664_Fig6_HTML.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.google.com/url?sa=i&url=https%3A%2F%2Fjournalofbigdata.springeropen.com%2Farticles%2F10.1186%2Fs40537-022-00664-6&psig=AOvVaw0oA70Csd6jdduMofUFmM8o&ust=1741022359189000&source=images&cd=vfe&opi=89978449&ved=0CBcQjhxqFwoTCOD80-vz64sDFQAAAAAdAAAAABAZ\">Image Credits</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above image shows simple representation of Encoder consisting of Bi-directional LSTM RNN. Lets understand the terminology first before proceeding. \n",
    "\n",
    "x(t-1), x(t), x(t+1) are the inputs at the respective time stamps. \n",
    "\n",
    "If we clearly observe the above inputs are passed to both the LSTM in forward and backward layers. Hence, the context will be retained in both the ways. We will receive a hidden state h(t-1), h(t) and h(t+1) from the LSTM neurons and the same will be passed to the subsequent LSTM for further processing.\n",
    "\n",
    "The final computation received after processing with the previous time stamp hidden state, will be passed to the sigmoid activation function and the y(t-1), y(t), y(t+1) are the final outputs at the respective time stamps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-time Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we remember, when a real human translator tries to translate the language fron one to another. He/she will translate three or four set of words at a time, instead of converting the whole sentence at a time. \n",
    "\n",
    "\n",
    "### In terms of our Attention model \n",
    "\n",
    "We have a term called \"Window Size\". During the initialization, we need to specify a Window size. The Window size is a hyperparameter, it can vary from 3, 4, 5, ...; It is denoted by T(x). \n",
    "\n",
    "Based on the window size, that many number of hidden states will be combined together and will represent one context vector. For example the first three hidden states (h1, h2 and h3) gets combined together and will represent Context vector (C1). \n",
    "\n",
    "Similarly, C2 will represent h2, h3 and h4; so on and so forth. Based on the window size the hidden states gets combined together and forms the context vector and the same will be passed to the Decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, weights plays a major role since based on the value of weights the model will know how much importance should be given to a particular word which is the main feature of Attention model. \n",
    "\n",
    "Weights are represented by $\\displaystyle\\alpha{t_{1}} $ which means the weight at timestamp = 1. The value of the weights will get initialized randomly. \n",
    "\n",
    "Remember everytimr the sum of all the weights in the window size should be equal to 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Vector Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The context vector at an instance (i) will be computed using the sum of product of weights multipled with the corresponding hidden state. For example, let us compute the value of C1.\n",
    " \n",
    "If we have initialized T(x) = 3 then,\n",
    "\n",
    "C(1) = $\\displaystyle\\alpha{t_{1}} $ * h(1) + $\\displaystyle\\alpha{t_{2}} $ * h(2) + $\\displaystyle\\alpha{t_{3}} $ * h(3) \n",
    "\n",
    "Similarly, \n",
    "\n",
    "C(2) = $\\displaystyle\\alpha{t_{2}} $ * h(2) + $\\displaystyle\\alpha{t_{3}} $ * h(3) + $\\displaystyle\\alpha{t_{4}} $ * h(4) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "source": [
    "The architecture consists of the following key components:  \n",
    "\n",
    "**Input Layer**  \n",
    "- The input is a **sequence** (e.g., words in a sentence or time-series data).  \n",
    "- Each word/time-step is represented as an **embedding vector** (for NLP) or numerical features (for time series).  \n",
    "\n",
    "**Bi-Directional LSTM (BiLSTM) Layer**  \n",
    "- **Two LSTM networks** process the sequence:  \n",
    "  - **Forward LSTM** → Reads input from left to right.  \n",
    "  - **Backward LSTM** → Reads input from right to left.  \n",
    "- The outputs from both LSTMs are **concatenated** to form a richer context representation.  \n",
    "\n",
    "Output of BiLSTM per time step → \\( h_t = [\\overrightarrow{h_t}, \\overleftarrow{h_t}] \\)  \n",
    "(Combination of forward and backward hidden states)  \n",
    "\n",
    "**Attention Mechanism**  \n",
    "- Instead of treating all time steps equally, **Attention assigns different importance** to each hidden state.  \n",
    "- Steps involved:  \n",
    "  1. Compute **alignment scores** between current hidden state and all previous states.  \n",
    "  2. Apply **softmax** to get attention weights.  \n",
    "  3. Compute the **weighted sum** of all hidden states to create the **context vector**.  \n",
    "  4. This **context vector** is used to make predictions.  \n",
    "\n",
    "**Context vector calculation:**  \n",
    "\n",
    "$\\ c_t = \\sum_{i} \\alpha_i h_i $\n",
    "\n",
    "where $\\ \\alpha_i $ are attention weights and $\\ h_i $ are BiLSTM outputs.  \n",
    "\n",
    "**Fully Connected (Dense) + Output Layer**  \n",
    "- The **context vector** is passed through a **fully connected (Dense) layer** with an activation function (e.g., softmax for classification).  \n",
    "- Final **output** is generated based on the attention-weighted context.  \n",
    "\n",
    "### **Key Benefits of This Architecture**  \n",
    "✅ BiLSTM captures both past & future context.  \n",
    "✅ Attention selectively focuses on important parts of the sequence.  \n",
    "✅ Improves performance in NLP and time-series tasks (e.g., translation, sentiment analysis, speech recognition).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
