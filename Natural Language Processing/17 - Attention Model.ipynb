{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before understanding about the attention model, lets understand about the drawbacks of the Encoders and Decoders. \n",
    "\n",
    "- Encoders and Decoders both work in a uni-directional way. \n",
    "\n",
    "- The context vector which is generated by the Encoder doesn't contain the whole context of the sentence in case if the sentence contains more words. \n",
    "- It will remember the context or memory of the sentences which are nearby only. \n",
    "- Fixed length context vector generation is eliminating the sentence importance leading in performance issues.\n",
    "\n",
    "Also if we discuss one more use case as in such, we have the below sentence and need to predict the word in the blank.  \n",
    "\n",
    "Krish likes to eat ______ in Bengaluru.\n",
    "\n",
    "In the above case, Encoder and Decoder may get failed because it has the context till 'likes to eat' but it doesn't have the context of 'in Bengaluru'. Before the prediction, our model should know the context of the forward words as well. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Attention Model is a mechanism that helps neural networks focus on the most relevant parts of an input sequence while making predictions. It dynamically assigns different importance (weights) to different words or tokens, improving tasks like translation, summarization, and sentiment analysis.\n",
    "\n",
    "Initially, the LSTMs in the Encoder - Decoders architecture are replaced by Bi-directional LSTM RNNs as it have the bi-directional data movement and preserve context of both forward and backward words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole architecture is modified as below: \n",
    "\n",
    "Encoder => Bi-directional LSTM RNN\n",
    "\n",
    "Decoder => LSTM RNN with minimal changes or no change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bi-directional LSTM RNN](https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs40537-022-00664-6/MediaObjects/40537_2022_664_Fig6_HTML.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.google.com/url?sa=i&url=https%3A%2F%2Fjournalofbigdata.springeropen.com%2Farticles%2F10.1186%2Fs40537-022-00664-6&psig=AOvVaw0oA70Csd6jdduMofUFmM8o&ust=1741022359189000&source=images&cd=vfe&opi=89978449&ved=0CBcQjhxqFwoTCOD80-vz64sDFQAAAAAdAAAAABAZ\">Image Credits</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above image shows simple representation of Encoder consisting of Bi-directional LSTM RNN. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
