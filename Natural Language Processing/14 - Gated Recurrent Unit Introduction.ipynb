{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gated Recurrent Unit (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gated Recurrent Unit(GRU) is a type of Recurrent Neural Network(RNN). It is similar to the LSTM(Long Short Term Memory) where it also has the capability of remembering or forgetting the information over time. \n",
    "\n",
    "When compared with LSTM, GRU has minimal operations hence its more fast to train and computationally efficient. \n",
    "\n",
    "In LSTM, the memory cell is dependent on three gates - Input gate, Forget gate and Output gate. But whereas coming to GRU, we only have two gates - Reset and Update gate, and the memory cell is replaced by the Candidate Activation Vector which is computed by using the current input and the reset gate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset gate\n",
    "\n",
    "Reset gate is computed using the help of current input and the previous hidden state. It tells how much of the previous hidden state to forget. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update gate\n",
    "\n",
    "Update gate is also computed using the current input and the previous hidden state, but update gate tells how much of the new information needs to be incorporated into the next/new hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
