{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-processing Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is defined as the processing of cleaning the text data before passing on to the model for training purpose. \n",
    "\n",
    "As we discussed earlier, machine can only understand binary language. So its our responsibility to convert the human text into machine understanble language like vectors and pass it on. There are various techniques to perform the same. \n",
    "\n",
    "In short the process will be like Raw Text => Words => Vectors\n",
    "\n",
    "So lets first discuss the **Steps involved in Text processing** here, where the following steps are involved. \n",
    "\n",
    "But before diving into the concept, let's understand about an ML use case example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider the use case of Spam Classification which can be implemented using Machine Learning and NLP. \n",
    "\n",
    "Here, our dataset contains the three columns: Email Subject, Email Body, Ouput (Spam/Ham) and some sample data is filled as well. \n",
    "\n",
    "| Email Subject | Email body | Output (Spam/Ham) | \n",
    "| :---: | :---: | :---: |\n",
    "| You've won lottery! | Claim it now | Spam |\n",
    "| Hi Kiran | How are you? | Ham |\n",
    "| You got Amazon coupon | Click here to open | Spam | \n",
    "\n",
    "If the above dataset is given to the model it will not be able to extract or understand the information given in the text form. It will not be able to predict as well. \n",
    "\n",
    "Hence, we first need to pre-process the text dataset using the Text pre-processing techniques of NLP and train the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps involved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the steps involved in text pre-processing:\n",
    "\n",
    "1. Tokenization\n",
    "\n",
    "2. Stopwords\n",
    "3. Stemming\n",
    "4. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first step performed in the text preprocessing. Tokenization is a process where it breaks the given sentence into multiple words called tokens. \n",
    "\n",
    "E.g., Sentence: *You've won 10,000 $$ lottery!*\n",
    "\n",
    "After tokenization it breaks into ['You've', 'won', '10,000', '$$', 'lottery!']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords are kind of words which mostly have less value or importance when compared with the other words in the whole sentence. These are often words like prepositions, articles, conjunctions, and common verbs.\n",
    "\n",
    "Some of the examples include: a, an, the, and, but, or, in, on, at, to, of, not, did, etc. \n",
    "\n",
    "Mostly, Stopwords are removed before moving to the next step, since it will make the model focus on the main sentence instead of the less important words. These words also contribute very less to the formation or meaning of the sentence. \n",
    "\n",
    "Also, we can create our own set or list of stop words if required and provide to the model for training purpose.\n",
    "\n",
    "E.g., \n",
    "\n",
    "| Original sentence | After removing Stop Words |\n",
    "| :---: | :---: |\n",
    "| Um, you know Microsoft is hosting its first AI conference! | Microsoft, hosting, first, AI, Conference |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is defined as a process of reverting/reducing the words to their original base (or) word stem (or) root word. \n",
    "\n",
    "E.g., Consider we have two words: \n",
    "\n",
    "| List of words | After stemming | \n",
    "| :---: | :---: | \n",
    "| Historical, History | Histori | \n",
    "| Finally, final, finalized | Fina | \n",
    "| going, goes, gone | go |\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Fast processing. \n",
    "- Overall time to transform dataset is very less.\n",
    "\n",
    "**Dis-advantages:**\n",
    "\n",
    "- The generated Root word or word stem is lacking the meaning of the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It solves the drawback of stemming, by using the help of dictionary before generating the output. It means once the word inputs are given, they get compared with a dictionary and then the output word is given. \n",
    "\n",
    "Lets re-consider the previous example again and observe the changes: \n",
    "\n",
    "| List of words | After lemmatization | \n",
    "| :---: | :---: | \n",
    "| Historical, History | History | \n",
    "| Finally, final, finalized | Final | \n",
    "| going, goes, gone | go |\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Meaningful word gets produced.\n",
    "\n",
    "**Dis-advantages:**\n",
    "\n",
    "- Slow processing and time taking as it needs to refer the dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Cases for Stemming: \n",
    "\n",
    "- Spam Classification\n",
    "\n",
    "- Review Classification\n",
    "\n",
    "## Use Cases for Lemmatization: \n",
    "\n",
    "- Text Summarization\n",
    "\n",
    "- Language Translation\n",
    " \n",
    "- Chatbot"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
