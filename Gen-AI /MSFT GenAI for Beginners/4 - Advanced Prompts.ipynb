{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we understood about prompts from scratch in the previous tutorial, now let us deep dive into the advanced version for the same. \n",
    "\n",
    "To keep it simple, **prompts** are nothing but a sentence which is given as input to the model to produce the results as expected. \n",
    "\n",
    "**Prompt Engineering** is the process of creating prompts which will produce the desired outcome. There are many techniques which we can use as part of prompt engineering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a basic prompt as shown below: \n",
    "\n",
    "*\"Generate 10 questions on geography\"*\n",
    "\n",
    "If we break it down, here we are using two type of prompt engineering techniques. \n",
    "\n",
    "1. **Context** - we are setting context as \"geography\".\n",
    "\n",
    "2. **Limiting the output** - mentioning only 10 lines is required.\n",
    "\n",
    "**Limitations:**\n",
    "But do you think, we will get the result as we expect. The answer is NO. Why? Because geography itself is a big subject so it can consider anything as it requires, and as output concerned the way of representation can be anything. \n",
    "\n",
    "But generative AI is capable of many things so lets have a glance of various prompt engineering techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniques for prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First and foremost, remember that Prompting is not something which the model is trained upon or something. Instead it is something which we need to use for discovering the model. \n",
    "\n",
    "Below are some of the techniques, that we can use to prompt an LLM. \n",
    "\n",
    "1. **Zero-shot prompting** - Most simplest form. No example is given only the prompt is given. \n",
    "\n",
    "2. **Few-shot prompting** - Helps in providing two or more examples in the prompt making the model respond accordingly. \n",
    "\n",
    "3. **Chain of thought** - type of prompting which guides the LLM in breaking down a problem into steps. \n",
    "\n",
    "4. **Generated knowledge** - by providing generated knowledge or facts additionally to our prompt helps in improving the LLM response. \n",
    "\n",
    "5. **Least to most** - Similar to chain of thought, this technique is breaking down problem into steps and then asking to perform the steps in the order. \n",
    "\n",
    "6. **Self-refine** - asking the LLM's output to refine itself means to improve the output better.\n",
    "\n",
    "7. **Maieutic Prompting** - validating the LLM response by asking it to provide an valid explanation of various part(s) of its answer.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Zero-shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Basic form of prompting which everyone or layman uses to interact with the LLM.\n",
    "\n",
    "- Example: What is algebra? Tell about India?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Few-shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Need to provide few examples in the prompt helping the LLM model to provide better response. \n",
    "\n",
    "- In simple words, its process of combining our prompt with few examples making the LLM model know to how to provide the output. \n",
    "\n",
    "- Examples: Write a poem in style of Shakespeare. Here are few examples \"Shall I compare thee to\", \"Sonnect 116: Let me not to the marriage\", etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Chain of thought "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Powerful technique in terms of Prompt Engineering.\n",
    "\n",
    "- Prompt is designed in such a way like it contains the detailed steps on how to solve a problem by breaking it down. \n",
    "\n",
    "- Consider we have provided a prompt to solve a Mathematical problem where BODMAS rule is applicable. But LLM is not aware about it and response provided is wrong. \n",
    "\n",
    "- By using Chain of thought technique, we will provide a problem and its solution using BODMAS by explaining the LLM how we divided the problem into steps and solved it. Now along with this, we will provide a fresh question. \n",
    "\n",
    "- Then LLM will refer to our breakdown of our problem and it does the same and provides the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generated Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When we are working for a company and we need to use the help of company data for prompting purpose then mostly we will use the prompt templates which we can populate the data at a later stage.\n",
    "\n",
    "    For example consider the below prompt: \n",
    "\n",
    "    *User name: {{username}}*\n",
    "\n",
    "    *Department: {{department}}*\n",
    "\n",
    "    *Monthly spendings: {{spendings_list}}`*\n",
    "\n",
    "    *Analyse and provide on which item the spendings are most?\n",
    "    Restrict your response only to the above spendings.*\n",
    "\n",
    "- Once the above prompt is populated with data using the API then we are combining the same with our prompt. \n",
    "\n",
    "- Here the knowledge or the content that we are passing is a generated knowledge or content and not a static one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Least-to-most"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Technique where we will break-down a bigger problem into sub problems and solve each of them sequentially.\n",
    "\n",
    "- Compared to Chain of thought prompting, in Least-to-most it uses the output of the previous subproblem as input for the next. \n",
    "\n",
    "- It provides higher accuracy when compared to Chain of thought."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Self-refine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Asking the LLM to critique itself or to refine its answer for more clarity.\n",
    "\n",
    "- For example, we have asked the LLM to create a Flask web app with products and cart routes. The response only contains the methods but not the whole template.\n",
    "\n",
    "- Then we will provide a prompt to LLM again asking the suggesstions to do in the code like add the __name__ = \"__main__\" to run the app, etc. \n",
    "\n",
    "- Self-refine includes the below simple steps:  \n",
    "   1. Provide initial prompt\n",
    "\n",
    "   2. LLM provides its response.\n",
    "   3. Ask the LLM to improve the response by critiquing it.\n",
    "   4. LLM answers again but this time more clear and considering the critique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Maieutic prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Its a prompting technique similar to the Self-refine. \n",
    "\n",
    "- Its more about asking the LLM to explain itself to validate its response by asking various questions and explanations.\n",
    "\n",
    "- Once we give the initial prompt, and LLM generates the response, ask questions in a deeper way to all the paragraphs generated and if it feels its inconsistent discard the same. \n",
    "\n",
    "- This way we can verify the output created by LLM and gain confidence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is a parameter which helps us to control the predictable nature/randomness/creativity of the model.\n",
    "\n",
    "- The value of the Temperature lies between 0 to 1. \n",
    "   - 0 means most likely or more consistent and conservative responses.\n",
    "\n",
    "   - 1 means most varied or more diverse or creative and completely imaginary outputs.\n",
    "- Default value of the temperature is set to 0.7\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Temperature value range varies from model to model. It mostly starts from 0 and sometimes have a higher range till 2. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
