{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Prompt Engineering\n",
    "\n",
    "In the era of Generative AI, prompts are something which plays a major role in the interaction with the LLM model. Lets understand from the sratch how the prompt engeering works and everything. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Prompt Engineering? \n",
    "\n",
    "Prompt Engineering is defined as the process of \"Designing\" and \"Optimizing\" text inputs a.k.a prompts to get the desired results from the model. By refining the prompts, we can make the model deliver consistent and well-formatted results. \n",
    "\n",
    "There is a 2-step process involved here:\n",
    "1. Initially, the prompt should be designed.\n",
    "\n",
    "2. Refining the prompt further to refine/improve the quality of the results.\n",
    "\n",
    "This is a trial-and-error process which requires user involvement and will be continuous until we get a desired output or result as we expect. \n",
    "\n",
    "But did we think why do we need to refine our prompts all the time? To understand the same, we need to first know about three important terms. \n",
    "\n",
    "1. **Tokenization** - how our \"prompt\" gets transformed\n",
    "\n",
    "2. **Base LLMs** - how the \"prompts\" gets processed further\n",
    "3. **Fine-tuned LLMs/Instruction-tuned LLMs** - how model is able to identify \"tasks\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each and every LLMs has its own different process of tokenization. \n",
    "\n",
    "Mostly, LLMs consider prompts like sequence of tokens combined together and breakdown of the given sentence into token varies from model to model. \n",
    "\n",
    "All the LLMs are trained on tokens, hence breaking the given sentence into tokens and passing on to the model gives us a better result. We can try playing with the available [OpenAI tokenizer](https://platform.openai.com/tokenizer?WT.mc_id=academic-105485-koreyst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Foundation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foundation models are also called as Base LLMs. These models main role is to just predict the next token in the sentence. \n",
    "\n",
    "Base LLMs doesnt have any understanding of the given token meaning. Since it is trained on the sentences, its role is just to predict the next words in the sequence. This process continues until and unless there is a human intervention or any specified condition mentioned to stop the response. \n",
    "\n",
    "If we give any information to the model, it will consider it as request and give the same output again in an formatted way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Instruction Tuned LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the LLMs which are built on top of the Foundational LLMs. The base models are getting fine-tuned with the training data and then those models will act like Instruction-tuned LLMs or Fine-tuned LLMs. \n",
    "\n",
    "Reinforcement Learning or Transfer learning is used to train the underlying model further with the company specific or new data. Hence, it results in better data and outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we need Prompt Engineering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Engineering plays an important role in overcoming the challenges the current LLMs possess in the market. In a high-level, below mentioned are the main three drawbacks:\n",
    "\n",
    "1. **Model responses stays same/varies when the same prompt executed** across different LLMs or the same model multiple times. \n",
    "\n",
    "2. **Model responses might be fabricated/hallucinated** when asked for an event which didn't happen or which is out of its training data. \n",
    "\n",
    "3. **Model capabilities vary from model to model**. Each model has its own differentations in terms of various factors. Be it the token limitation or costs it acts like a differentiator.\n",
    "\n",
    "### Lets learn more about the term \"Fabrication\" \n",
    "\n",
    "Fabrication is the perfect term which can be used instead of \"Hallucinations\". Hallucinations is a term used for more of a human like behaviour, but it does suit when we are referring in term of machine. \n",
    "\n",
    "The model responses gets fabricated when the model doesnt know about the prompt that we have given. Instead of saying 'No', instead it will fabricate the response as in a way convincing the users that the event has occurred.\n",
    "\n",
    "### Steps to avoid Fabrication/Hallucination\n",
    "- Asking the model for the citations or references for the generated information will let the user validate the response.\n",
    "- Temperature Configuration may reduce model fabrications.\n",
    "\n",
    "- Metaprompting is a process where instructions are designed to help LLMs create more precise and focused prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction of Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, the way prompts are constructed matters a lot as it defines the output. \n",
    "\n",
    "Let us see various ways which are used in terms of Prompt Engineering: \n",
    "\n",
    "1. **Basic Prompt** - sentence without no context passed to a model. The model tries to complete the sentence with its own information.\n",
    "\n",
    "   **Example:** Oh say you can see - *model simply tries to add some information to complete the sentence.*\n",
    "\n",
    "\n",
    "2. **Complex Prompt** - Whenever our prompt is combined with System information providing more context along with the Input/Output pairs reflecting user input and corresponding responses. \n",
    "\n",
    "   **Example:** Setting the role and system information - `{ \"role\" : \"system\", \"content\" : \"You are a helpful AI assistant\"}`\n",
    "\n",
    "\n",
    "3. **Instruction Prompt** - Giving instructions such as give the output in JSON format, provide output in a tablular format, etc. \n",
    "\n",
    "   **Example:** Provide the output in a list format, JSON format, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primary Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primary Content is a design pattern where the input text or prompt is given in such a way that it contains two parts: \n",
    "\n",
    "- Relevant Content/Data \n",
    "- Instruction or Action \n",
    "\n",
    "Example:  \n",
    "\n",
    "| Relevant content | Instruction/Action |\n",
    "| :----: | :---: |\n",
    "| <<Text content containing 5 paragraphs>> | Summarize content into 2 lines |\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tweak this Primary content in multiple ways to effectively improve our response. Some of the main ways are listed below: \n",
    "\n",
    "- Providing Examples \n",
    "\n",
    "- Using cues or hints \n",
    "- Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Providing Examples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we will use the help of examples to let know the model what we are expecting in return exactly. This helps the model process and return the response in the same fashion what we have given inside. \n",
    "\n",
    "Based on the number of examples, we have three types of prompting which are defined: \n",
    "\n",
    "1. **Zero-shot prompting:** When zero or no examples are provided in the prompt. \n",
    "\n",
    "2. **One-shot prompting:** One example is provided in our prompt.\n",
    "3. **Few-shot prompting:** Two or more examples are provided in our prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Cues/Hints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than examples, we can use cues or hints in our prompts. For example, consider we have given a paragraph as input and asking the model to summarize. \n",
    "\n",
    "Usually we will give the prompt like \n",
    "\n",
    "**Summarize this in 2 lines**\n",
    "\n",
    "But if we make use of cues/hints then we can ask the model to start with this particular word. For example, let me modify the same prompt which is written ahead:\n",
    "\n",
    "**Summarize this**\n",
    "**Top 3 facts we learned**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the name represents, all the prompts are written and stored in the form of templates which usually contain \n",
    "\n",
    "Role - system,user,doctor,etc.\n",
    "System information - You are helpful assistant\n",
    "Prompts containing placeholders.\n",
    "\n",
    "Mostly these prompt templates are already designed and can be used Programatically by just replacing the already present placeholders with some content which we took as the input from user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Chat Prompt Template from langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful IT assistant\"),\n",
    "    (\"user\", \"Tell me a joke about {topic}\"),\n",
    "])\n",
    "\n",
    "# invoking the prompt template\n",
    "prompt = prompt_template.render(topic=\"computers\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
