{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Prompt Engineering\n",
    "\n",
    "In the era of Generative AI, prompts are something which plays a major role in the interaction with the LLM model. Lets understand from the sratch how the prompt engeering works and everything. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Prompt Engineering? \n",
    "\n",
    "Prompt Engineering is defined as the process of \"Designing\" and \"Optimizing\" text inputs a.k.a prompts to get the desired results from the model. By refining the prompts, we can make the model deliver consistent and well-formatted results. \n",
    "\n",
    "There is a 2-step process involved here:\n",
    "1. Initially, the prompt should be designed.\n",
    "\n",
    "2. Refining the prompt further to refine/improve the quality of the results.\n",
    "\n",
    "This is a trial-and-error process which requires user involvement and will be continuous until we get a desired output or result as we expect. \n",
    "\n",
    "But did we think why do we need to refine our prompts all the time? To understand the same, we need to first know about three important terms. \n",
    "\n",
    "1. **Tokenization** - how our \"prompt\" gets transformed\n",
    "\n",
    "2. **Base LLMs** - how the \"prompts\" gets processed further\n",
    "3. **Fine-tuned LLMs/Instruction-tuned LLMs** - how model is able to identify \"tasks\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each and every LLMs has its own different process of tokenization. \n",
    "\n",
    "Mostly, LLMs consider prompts like sequence of tokens combined together and breakdown of the given sentence into token varies from model to model. \n",
    "\n",
    "All the LLMs are trained on tokens, hence breaking the given sentence into tokens and passing on to the model gives us a better result. We can try playing with the available [OpenAI tokenizer](https://platform.openai.com/tokenizer?WT.mc_id=academic-105485-koreyst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Foundation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foundation models are also called as Base LLMs. These models main role is to just predict the next token in the sentence. \n",
    "\n",
    "Base LLMs doesnt have any understanding of the given token meaning. Since it is trained on the sentences, its role is just to predict the next words in the sequence. This process continues until and unless there is a human intervention or any specified condition mentioned to stop the response. \n",
    "\n",
    "If we give any information to the model, it will consider it as request and give the same output again in an formatted way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Instruction Tuned LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the LLMs which are built on top of the Foundational LLMs. The base models are getting fine-tuned with the training data and then those models will act like Instruction-tuned LLMs or Fine-tuned LLMs. \n",
    "\n",
    "Reinforcement Learning or Transfer learning is used to train the underlying model further with the company specific or new data. Hence, it results in better data and outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we need Prompt Engineering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Engineering plays an important role in overcoming the challenges the current LLMs possess in the market. In a high-level, below mentioned are the main three drawbacks:\n",
    "\n",
    "1. **Model responses stays same/varies when the same prompt executed** across different LLMs or the same model multiple times. \n",
    "\n",
    "2. **Model responses might be fabricated/hallucinated** when asked for an event which didn't happen or which is out of its training data. \n",
    "\n",
    "3. **Model capabilities vary from model to model**. Each model has its own differentations in terms of various factors. Be it the token limitation or costs it acts like a differentiator.\n",
    "\n",
    "### Lets learn more about the term \"Fabrication\" \n",
    "\n",
    "Fabrication is the perfect term which can be used instead of \"Hallucinations\". Hallucinations is a term used for more of a human like behaviour, but it does suit when we are referring in term of machine. \n",
    "\n",
    "The model responses gets fabricated when the model doesnt know about the prompt that we have given. Instead of saying 'No', instead it will fabricate the response as in a way convincing the users that the event has occurred.\n",
    "\n",
    "### Steps to avoid Fabrication/Hallucination\n",
    "- Asking the model for the citations or references for the generated information will let the user validate the response.\n",
    "- Temperature Configuration may reduce model fabrications.\n",
    "\n",
    "- Metaprompting is a process where instructions are designed to help LLMs create more precise and focused prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction of Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, the way prompts are constructed matters a lot as it defines the output. \n",
    "\n",
    "Let us see various ways which are used in terms of Prompt Engineering: \n",
    "\n",
    "1. **Basic Prompt** - sentence without no context passed to a model. Eg. Oh say you can see - in this case the model tries to complete the sentence with its own information.\n",
    "\n",
    "2. **Complex Prompt** - Whenever our prompt is combined with System information providing more context along with the Input/Output pairs reflecting user input and corresponding responses. \n",
    "\n",
    "3. **Instruction Prompt** - Giving instructions such as give the output in JSON format, provide output in a tablular format, etc. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
