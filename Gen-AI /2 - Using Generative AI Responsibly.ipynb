{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Generative AI responsibly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the excitement of Generative AI is higher, it is of utmost importance we need to focus on the output of the models. The output of the Generative AI should be responsbile and should not contain any harmful content or hate content, etc. \n",
    "\n",
    "As a developer we need to focus on the principles of Responsible AI and implement it while developing any GenAI application. Lets have a glance on over the principles: \n",
    "\n",
    "- Fairness\n",
    "\n",
    "- Inclusiveness\n",
    "- Reliability/Safety\n",
    "- Security and Privacy\n",
    "- Transparency \n",
    "- Accountability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why prioritizing Responsible AI is important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative AI played a major role in helping users to get creative answers, summarize documents, etc. without any manual intervention. \n",
    "\n",
    "But there are chances that without proper planning the model may provide the answers which might contain harmful content and not suitable to consume by users. Below mentioned are some of the risks/harmful results identified so far: \n",
    "\n",
    "### - Hallucinations\n",
    "\n",
    "This is a term used to describe when an LLM produces content which is non-sensical or factually incorrect. In short, it sometimes produces the information which was not happend anywhere but it shows as it happened. \n",
    "\n",
    "With each iteration of LLM, the hallucinates were improved but as a developer we need to make sure it doesn't display such answers as it affect user satisfaction. \n",
    "\n",
    "### - Harmful Content\n",
    "\n",
    "LLM models may also return responses containing harmful content such as instructions to perform any harmful task, hateful or demeaning content, guiding planning an attach or displaying some illegal content which is completely unauthorized. \n",
    "\n",
    "We need to use strategies to eliminate such type of responses from our model to avoid any such instances occurring in future.\n",
    "\n",
    "### - Lack of fairness\n",
    "\n",
    "Fairness means the response should be transparent and without any bias and discrimination. This will make the model work common for everyone and does not affect any self-experience of the users. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to make our GenAI Solutions work responsibly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a high-level there are 4 steps where we can leverage to develop our AI solutions responsibly. They are:\n",
    "\n",
    "**1. Identify:**\n",
    "Process of identifying such instances where the AI model is behaving in an unexpected manner.\n",
    "\n",
    "**2. Measure:**\n",
    "Based on the use case our model is intended to use, we need to prepare some set of prompts (just like test-cases for Software testing) and run the prompts aganist our model to check the content. \n",
    "\n",
    "**3. Mitigate:** \n",
    "Once we get the results from our previous step, this is the time where we need to eliminate such information from our model such that it will be prevented to show such type of outputs to the users. To perform the same, we have four types of layers in place: \n",
    "\n",
    "- Model - It all starts with the right LLM model that we choose. GPT 4.0 or higher can cause higher risk as its a large model. Instead if required we can fine-tune the model to modify our responses.\n",
    "\n",
    "- Safety System - This layer does the content filtering and also capable of detecting any jailbreak attacks and unwanted activity like requests from bots. \n",
    "\n",
    "- Metaprompt and Grounding - These are two ways where we can limit the model or direct the model its working. We can use system inputs to do define the limits. Retrievel Augumented Generation (RAG) can be used here to ground the responses as it will consider the information from our Vector database. \n",
    "\n",
    "- Evalulate model - As we are using LLMs we dont have the visibility of the training data. But still as a developer we need to evaluate the model performance, accuracy, similariy, groundedness and relevance of the output. \n",
    "\n",
    "**4. Operate:**\n",
    "\n",
    "This is the final layer where its recommended to partner with startups or security companies which ensure we are completely compliant with all the regulatory policies. We need to develop various strategies for the delivery, handling incidents and make our model behave responsibly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Azure AI Content Safety** is an AI service where we can employ it to detect harmful user or AI generated content.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
