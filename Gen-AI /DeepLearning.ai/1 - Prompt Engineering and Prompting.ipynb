{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of developing, improving and refining the prompt is called as Prompt Engineering. But before understanding the same, lets us have a look at some basic terminology in terms of Prompts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt - The text which is feed into the model is called as Prompt.\n",
    "\n",
    "- Inference - The act of text generation by the LLM is called as Inference. \n",
    "- Completion - The output of the LLM is known as Completion. \n",
    "- Context Window - The full amout of text or memory which can be used for the prompt is called as Context Window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-context Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We all have encountered at some point of time, that LLM wont produce responses as expected or sometimes might be completely irrelevant. To avoid this we can provide the Examples inside the prompt to make the LLM understand more better. \n",
    "\n",
    "This process of providing examples inside our prompt to make LLM better understand is termed as \"In-context Learning\". Using this we can provide more examples or extra data which the LLM can use to provide better relevant response. \n",
    "\n",
    "## Types of Inferences \n",
    "Consider an usecase where we are asking an LLM to predict the sentiment based on the given prompt. Using the in-context learning, there were three types of inferences:\n",
    "\n",
    "### 1. Zero-shot Inference\n",
    "If we directly give our prompt and expect a response without giving any examples. \n",
    "\n",
    "E.g., `REVIEW: Its very tasty!  SENTIMENT: ?`\n",
    "\n",
    "\n",
    "### 2. One-shot Inference \n",
    "The inclusion of a single example in our prompt is called One-shot Inference. Here, we will make our prompt longer with one example of the sentence that we are expecting. \n",
    "\n",
    "E.g., `REVIEW: Its very tasty!  SENTIMENT: Positive`\n",
    "      `REVIEW: It tastes salty  SENTIMENT: ?`     \n",
    "\n",
    "\n",
    "### 3. Few-shot Inference \n",
    "The inclusion of two or more examples in our prompt is called Few-shot Inference. Here, we will make our prompt longer with two or more examples to make the LLM understand better. \n",
    "\n",
    "E.g., `REVIEW: Its very tasty!  SENTIMENT: Positive`\n",
    "      `REVIEW: It tastes salty SENTIMENT: Negative`\n",
    "      `REVIEW: Its very tasty!  SENTIMENT: ?`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most cases, large models are always good at Zero-shot Inference, but small models often needs more context to provide good responses. Hence, smaller models can benefit from One-shot or Few-shot inferences.\n",
    "\n",
    "- We can also add five or six examples in our prompt until our context window supports the same. \n",
    "- We can refine and refine our prompt until we get the expected answer if not, we need to Fine-tune the model to make it align with our responses which we will understand or learn in the upcoming topics."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
