{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Methods to Optimize the LLM model for Deployment"]},{"cell_type":"markdown","metadata":{},"source":["Large Language Models (LLMs) present inference challenges in terms of computing and storage requirements, and low latency for consuming applications. \n","\n","These challenges remains the same even when you're deploying on-premises or cloud and gets more difficult when deploying to edge devices.\n","\n","> Primary way to improve the Application performance is to reduce the LLM model size which allows quicker loading of model and reduces inference latency. \n","\n","But there is a challenge here... We need to ensure the model performance is not decreased when the model size is decreased. To overcome the same, we have three types of techniques which can be leveraged.\n","\n","### Types of Techniques:\n","\n","1. Distillation - teacher model trains a student model\n","\n","2. Quantization - reduces the dimension from 16-bit to 8-bit floating point\n","3. Pruning - removes the duplicate/redundant parameters or weights in the model"]},{"cell_type":"markdown","metadata":{},"source":["## Distillation "]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":2}
