{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Methods to Optimize the LLM model for Deployment"]},{"cell_type":"markdown","metadata":{},"source":["Large Language Models (LLMs) present inference challenges in terms of computing and storage requirements, and low latency for consuming applications. \n","\n","These challenges remains the same even when you're deploying on-premises or cloud and gets more difficult when deploying to edge devices.\n","\n","> Primary way to improve the Application performance is to reduce the LLM model size which allows quicker loading of model and reduces inference latency. \n","\n","But there is a challenge here... We need to ensure the model performance is not decreased when the model size is decreased. To overcome the same, we have three types of techniques which can be leveraged.\n","\n","## Types of Techniques:\n","\n","1. Distillation - teacher model trains a student model\n","\n","2. Quantization - reduces the dimension from 16-bit to 8-bit floating point\n","3. Pruning - removes the duplicate/redundant parameters or weights in the model"]},{"cell_type":"markdown","metadata":{},"source":["### 1. Distillation \n","\n","- This is a technique where a larger teacher model trains a smaller student model. \n","\n","- The student model learns to statistically mimic the behaviour of the teacher model either in the final prediction or hidden layers as well. \n","\n","- Imagine two circles, big cirle (Teacher Model which is fine-tuned LLM) is connected to a small circle (Student model). Knowledge distillation is the technique which is used to transfer the knowledge from Teacher to Student model. \n","\n","    **Note:** No reduction in the LLM Size."]},{"cell_type":"markdown","metadata":{},"source":["### 2. Quantization \n","\n","- As we learned before about this Quantization technique, it reduces the 16-bit Floating point representation to 8-bit floating point representation resulting in vast reduction of the model size. \n","\n","- Post-Training Quantization (PTQ) reduces the precision of the model weights by reducing the size of the LLM model. \n","\n","    **Note:** Model Size will be reduced vastly. \n"]},{"cell_type":"markdown","metadata":{},"source":["### 3. Pruning\n","\n","- Removes the model weights which are zero or very close to zero. \n","\n","- We can leverage some techniques such as Full model re-training, PEFT/LoRA and Post-training to remove weights which are zero or near to zero. \n","\n","**Note:** As per theory, this technique reduces the model size and hence performance. "]}],"metadata":{"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":2}
