{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LLM in Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the steps are complete and our model is good to be integrated with our Application, LLM model's still might face some difficulties. Some of them are listed below: \n",
    "\n",
    "- Out of data Knowledge\n",
    "\n",
    "- Hallucinations/Fabricated responses in Completions\n",
    "- Unable to do Basic Mathematical operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While these problems directly impacts the User experience of our App, we can solve this by integrating our LLM model with external applications (APIs) and external sources, etc. by using the below framework named RAG in short. \n",
    "\n",
    "# Retrieval Augumented Generation (RAG) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RAG is a framework which helps in solving the knowledge cut-off issue. \n",
    "\n",
    "- We can still re-train or fine-tune the model which is quite expensive, instead we can leverage the functionality of RAG in less cost. \n",
    "\n",
    "- RAG provides the model with information gathered from external sources, applications, etc.\n",
    "\n",
    "- It helps the LLM with data which it was not aware during its training. \n",
    "\n",
    "    Eg. *Who is the Prime Minister? - Manmohan Singh (Wrong answer but this is what LLM was trained)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Once the user provides its input prompt, the Query Encoder will encode it into vectors.\n",
    "\n",
    "2. This Query Encoder will be connected to External Information sources which is termed as Vector Database.\n",
    "3. This prompt will be sent to Vector database and related information will be fetched from the Vector Database. \n",
    "4. Now, along with the prompt the extracted information will be combined and sent to the LLM for further processing. \n",
    "5. Finally LLM by referring with the latest information will be able to provide better crafted accurate response. \n",
    "\n",
    "Usually External Information sources include - Documents, Wikis, Expert Systems, Web pages, Databases and Vector Stores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Vector Store?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A specialized library focused on efficiently storing and searching vectors. \n",
    "\n",
    "- It often provides indexing techniques and similarity search functionalities, enabling faster retrieval of similar data points. \n",
    "\n",
    "- All the prompts and external data sources are converted into vector embeddings and stored in a form of vector store which later will be used for faster retrieval. \n",
    "\n",
    "### How it works?\n",
    "\n",
    "The user input (prompt) will be converted to Embedding vectors, similarly all the external information sources will be broken down into chunks and then converted to embedding vectors similar to before. \n",
    "\n",
    "Now, Cosine similarity is used to identify the nearest vector embedding corresponding to prompt which will be returned as a response. \n",
    "\n",
    "Each text in the vector store is identified as key, and it enables a citation to be included in final completion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> RAG framework makes the LLM model avoid hallucinations in its completion."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
