{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain with Memory\n",
    "\n",
    "- ConversationBufferMemory\n",
    "\n",
    "- ConversationBufferWindowMemory\n",
    "- ConversationTokenBufferMemory\n",
    "- ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Environment variables file\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory\n",
    "\n",
    "Stores all the conversation incrementally from the start in form of a buffer memory, which will be passed as context when the llm is invoked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: Hi I am Sai Kiran!\\nAI: Hi Sai, how may I help you!\\nHuman: What is 1+1?\\nAI: The answer is 2'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain # Deprecated as of 2025 and replaced with \"RunnableWithMessageHistory\"\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "conversationBufferMemory = ConversationBufferMemory() # Creating an object \n",
    "\n",
    "# Likewise once we interact with the LLM, the context gets saved incrementally\n",
    "conversationBufferMemory.save_context({\"input\" : \"Hi I am Sai Kiran!\"}, { \"output\" : \"Hi Sai, how may I help you!\"})\n",
    "conversationBufferMemory.save_context({\"input\" : \"What is 1+1?\"}, { \"output\" : \"The answer is 2\"})\n",
    "\n",
    "# Displays the complete context\n",
    "print(conversationBufferMemory.load_memory_variables({}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory\n",
    "\n",
    "Similar to `ConversationBufferMemory`, but it stores the latest conversation based on the provided `k` value during instantiation. \n",
    "\n",
    "For example, there are around 10 turns of conversation happened, and if we initialize the value of `k = 2`, the context will be trimmed to last 2 conversations only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: How it is going?\\nAI: Its going better.\\nHuman: Hey!\\nAI: I am good.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain # Deprecated as of 2025 and replaced with \"RunnableWithMessageHistory\"\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "conversationBufferWindowMemory = ConversationBufferWindowMemory(k=2)\n",
    "\n",
    "# Likewise once we interact with the LLM, the context gets saved incrementally\n",
    "conversationBufferWindowMemory.save_context({\"input\" : \"Hi I am Sai Kiran!\"}, { \"output\" : \"Hi Sai, how may I help you!\"})\n",
    "conversationBufferWindowMemory.save_context({\"input\" : \"What is 1+1?\"}, { \"output\" : \"The answer is 2\"})\n",
    "conversationBufferWindowMemory.save_context({\"input\" : \"How it is going?\"}, { \"output\" : \"Its going better.\"})\n",
    "conversationBufferWindowMemory.save_context({\"input\" : \"Hey!\"}, { \"output\" : \"I am good.\"})\n",
    "\n",
    "# Displays the context showing only the latest k conversation\n",
    "print(conversationBufferWindowMemory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationTokenBufferMemory\n",
    "\n",
    "Instead of using the value of `k`, the conversation memory will be limited to the number of tokens as specified while instantating an object. It means, if 100 is the token limit we set, automatically the older or earlier tokens will be truncated acccordingly irrespective of what content is present before.\n",
    "\n",
    "This memory will be useful when there are strict restrictions on Token Usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'AI: The answer is 2\\nHuman: How it is going?\\nAI: Its going better.\\nHuman: Hey!\\nAI: I am good.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain # Deprecated as of 2025 and replaced with \"RunnableWithMessageHistory\"\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "\n",
    "conversationTokenBufferMemory = ConversationTokenBufferMemory(\n",
    "                                    llm=llm, # LLM will be passed as token size differs from each LLM to LLM\n",
    "                                    max_token_limit=30) # Token limit explicitly set.\n",
    "\n",
    "# Likewise once we interact with the LLM, the context gets saved incrementally\n",
    "conversationTokenBufferMemory.save_context({\"input\" : \"Hi I am Sai Kiran!\"}, { \"output\" : \"Hi Sai, how may I help you!\"})\n",
    "conversationTokenBufferMemory.save_context({\"input\" : \"What is 1+1?\"}, { \"output\" : \"The answer is 2\"})\n",
    "conversationTokenBufferMemory.save_context({\"input\" : \"How it is going?\"}, { \"output\" : \"Its going better.\"})\n",
    "conversationTokenBufferMemory.save_context({\"input\" : \"Hey!\"}, { \"output\" : \"I am good.\"})\n",
    "\n",
    "# Displays the context showing based on the token limit set\n",
    "print(conversationTokenBufferMemory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationSummaryBufferMemory\n",
    "\n",
    "Similar to before memory, here also we will set the `max_token_limit` to some value. But instead of truncating the previous conversation directly, when the `load_memory_variables({})` method is called, LLM generates a summary based on the conversation and it makes sures the conversation (summary + recent raw messages) is in adherence to the `max_token_limit`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'System: The human greets the AI. The AI responds. The human states they are not doing much. The AI acknowledges this. The human asks the AI about its schedule.\\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain # Deprecated as of 2025 and replaced with \"RunnableWithMessageHistory\"\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "# creating a long string containing a summary\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "conversationSummaryBufferMemory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "\n",
    "conversationSummaryBufferMemory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "conversationSummaryBufferMemory.save_context({\"input\": \"Not much, just hanging\"}, {\"output\": \"Cool\"})\n",
    "conversationSummaryBufferMemory.save_context({\"input\": \"What is on the schedule today?\"}, {\"output\": f\"{schedule}\"})\n",
    "\n",
    "# Displays the summary + conversation in adherence to the max_token_limit that was set. \n",
    "print(conversationSummaryBufferMemory.load_memory_variables({}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
