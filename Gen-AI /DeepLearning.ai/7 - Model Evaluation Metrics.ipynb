{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Metrics\n",
    "\n",
    "The following are two main metrics used to evaluate the performance of the LLM model. \n",
    "\n",
    "We have metrics such as Confusion Matrix, Recall, Precision, F1 Score, etc. which we used in Machine Learning models to calculate the accuracy and performance.\n",
    "\n",
    "In context of LLM, our input and output data is Text which the above mentioned metrics will not be suitable to calculate. Hence, we have mainly two types of metrics that are defined by researchers. They are: \n",
    "\n",
    "1. BLEU Score (Bi-lingual Evaluation for understanding)\n",
    "\n",
    "2. ROUGE (Recall oriented Understanding for Gisting evaluation)\n",
    "\n",
    "\n",
    "| BLEU Score | ROUGE | \n",
    "| :---: | :---: | \n",
    "| Evalulates Text Translation | Evalulates Text Summarization |\n",
    "| Compares with human-generated translations | Compares a summary to one or more reference summaries |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites \n",
    "\n",
    "Before understanding deeper about both the metrics, lets have a look about the terminology - unigram, bigram and n-grams. \n",
    "\n",
    "Consider we were given a sentence - \"The dog lay on the rug as I sipped cup of tea\" \n",
    "\n",
    "- **Unigram(s)** - The, dog, lay, on, the, rug, ..., tea.\n",
    "\n",
    "- **Bigram(s)** - (The dog), (lay on), (the rug), (of tea). \n",
    "- **n-gram(s)** - (The dog lay), (on the rug), (as I sipped)\n",
    "\n",
    "\n",
    "**Reference text** - Human generated sentence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU Score (Bi-lingual Evaluation for Understanding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is one of the evaluation metric used to evaluate the performance of the LLM. \n",
    "\n",
    "- This score works on evaulating the \"Text Translation\" feature of the LLM. It then compares with the original human generated translations. \n",
    "\n",
    "- BLEU Score is calculated by simply evaluating the \"Number of N-grams in machine generated translation matched with the reference sentence (human generated)\". \n",
    "\n",
    "- We can use Hugging face libraries which has the in-built functiionality of calculating BLEU score directly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE (Recall oriented Understanding for Gisting Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, ROUGE Score is used to evalulate Text Summarization of the LLM. \n",
    "\n",
    "To caclulate the ROUGE Score, we need to use three formulae specifically for Unigrams, Bi-grams and N-grams.\n",
    "\n",
    "### ROUGE-1 for Unigrams\n",
    "\n",
    "ROUGE-1 Recall = (Unigram matches) / (Unigrams in reference)\n",
    "\n",
    "ROUGE-1 Precision = (Unigram matches) / (Unigrams in output)\n",
    "\n",
    "ROUGE-1 F1 = 2 * (Precision) * (Recall) / (Precision + Recall) \n",
    "\n",
    "### ROUGE-2 and ROUGE-L\n",
    "\n",
    "Similarly, the Unigrams is replaced by bi-grams (ROUGE-2), and n-grams (ROUGE-L) and repeat the above calculations. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
