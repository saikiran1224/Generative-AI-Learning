{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReAct Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ReAct is a framework that combines reasoning with acting. It was designed to improve the accuracy and reliability of large language models (LLMs) by allowing them to dynamically reason, take actions, and observe the results of those actions.\n",
    "\n",
    "- This iterative process, imitates how humans solve problems, helps LLMs make more informed decisions and generate more relevant output.\n",
    "\n",
    "- ReAct agents can use search engines, databases, and other resources to gather information and refine their responses.\n",
    "\n",
    "- Start's with a thought (e.g., a complex question), take an action (e.g., a search), observe the result, and then use the result to further refine the reasoning and decide on the next action to be taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works? \n",
    "\n",
    "The whole process is completely Iterative. ReAct operates on a cycle of thinking, acting, observing, and adapting.\n",
    "\n",
    "- **Step 1 - Reasoning** - LLM uses its language understanding capabilities to analyze the problem and develop a strategy.\n",
    "\n",
    "- **Step 2 - Acting (Action Planning and Execution):** Based on its reasoning, LLM plans and executes specific actions, such as fetching data from external sources.\n",
    "\n",
    "- **Step 3 - Observing:** LLM observes the results of its actions and uses this information to refine its reasoning and plan its next actions. \n",
    "\n",
    "- **Step 4 - Adapting:** LLM adapts its strategy based on the observations it makes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Note: PAL and ReAct frameworks works well when used with Large models having atleast more than 1 Billion parameters since they have a higher context window and good understanding capability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LangChain is an open-source framework which is primarily used to simplify the development of applications that leverage Large Language Models (LLMs). \n",
    "\n",
    "- It provides tools and abstractions to make it easier to connect LLMs with external data sources and other applications, allowing developers to build more complex and context-aware AI applications. \n",
    "\n",
    "- LangChain provides a standard interface for interacting with various LLMs, making it easier to switch between different models or integrate them into existing workflows. \n",
    "\n",
    "- Langchain has three main components - Prompt Template, Tools and Memory combinely called as \"Chain\". \n",
    "\n",
    "- The main feature of Langhchain is its Tools. It provides lot of tools which we can directly use in our application without thinking about the integration, etc. with our application. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation using Gemini 2.0 LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU \"langchain[google-genai]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading GEMINI_API_KEY from environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os \n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing GEMINI chat model from Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello there! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--6343535f-4970-4647-bccc-d5af3f9ae67a-0', usage_metadata={'input_tokens': 3, 'output_tokens': 11, 'total_tokens': 14, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")\n",
    "\n",
    "model.invoke(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Messages from `langchain-core`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='హలో! (Halo!)' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--7789236a-9d02-4339-897e-7b14b952ddd4-0' usage_metadata={'input_tokens': 10, 'output_tokens': 7, 'total_tokens': 17, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "systemMessage = SystemMessage(\"Translate user instructions from English to Telugu.\")\n",
    "humanMessage = HumanMessage(\"Hello!\")\n",
    "\n",
    "response = model.invoke([systemMessage, humanMessage])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='Convert the provided text from English to Telugu.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Read the provided - Hello!', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "system_message = \"Convert the provided text from English to {language}.\"\n",
    "\n",
    "human_message = \"Read the provided - {text}\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_message), (\"human\", human_message)]\n",
    ")\n",
    "\n",
    "print(prompt_template.format_messages(language=\"Telugu\", text=\"Hello!\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
