{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReAct Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ReAct is a framework that combines reasoning with acting. It was designed to improve the accuracy and reliability of large language models (LLMs) by allowing them to dynamically reason, take actions, and observe the results of those actions.\n",
    "\n",
    "- This iterative process, imitates how humans solve problems, helps LLMs make more informed decisions and generate more relevant output.\n",
    "\n",
    "- ReAct agents can use search engines, databases, and other resources to gather information and refine their responses.\n",
    "\n",
    "- Start's with a thought (e.g., a complex question), take an action (e.g., a search), observe the result, and then use the result to further refine the reasoning and decide on the next action to be taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works? \n",
    "\n",
    "The whole process is completely Iterative. ReAct operates on a cycle of thinking, acting, observing, and adapting.\n",
    "\n",
    "- **Step 1 - Reasoning** - LLM uses its language understanding capabilities to analyze the problem and develop a strategy.\n",
    "\n",
    "- **Step 2 - Acting (Action Planning and Execution):** Based on its reasoning, LLM plans and executes specific actions, such as fetching data from external sources.\n",
    "\n",
    "- **Step 3 - Observing:** LLM observes the results of its actions and uses this information to refine its reasoning and plan its next actions. \n",
    "\n",
    "- **Step 4 - Adapting:** LLM adapts its strategy based on the observations it makes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Note: PAL and ReAct frameworks works well when used with Large models having atleast more than 1 Billion parameters since they have a higher context window and good understanding capability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LangChain is an open-source framework which is primarily used to simplify the development of applications that leverage Large Language Models (LLMs). \n",
    "\n",
    "- It provides tools and abstractions to make it easier to connect LLMs with external data sources and other applications, allowing developers to build more complex and context-aware AI applications. \n",
    "\n",
    "- LangChain provides a standard interface for interacting with various LLMs, making it easier to switch between different models or integrate them into existing workflows. \n",
    "\n",
    "- Langchain has three main components - Prompt Template, Tools and Memory combinely called as \"Chain\". \n",
    "\n",
    "- The main feature of Langhchain is its Tools. It provides lot of tools which we can directly use in our application without thinking about the integration, etc. with our application. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Simple AI Chat Assistant using Gemini 2.0 LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU \"langchain[google-genai]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading GEMINI_API_KEY from environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os \n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing GEMINI chat model from Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello there! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--4bbfb16a-70eb-4f18-8e52-3fbb1d16d4dd-0', usage_metadata={'input_tokens': 3, 'output_tokens': 11, 'total_tokens': 14, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")\n",
    "\n",
    "model.invoke(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Messages from `langchain-core`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='నమస్కారం! (Namaskaram!)\\n\\nOr, more informally:\\n\\nహలో! (Hello!)' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--1fdd9ac3-92f4-42e2-ae21-5e34a1dc3604-0' usage_metadata={'input_tokens': 10, 'output_tokens': 26, 'total_tokens': 36, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "systemMessage = SystemMessage(\"Translate user instructions from English to Telugu.\")\n",
    "humanMessage = HumanMessage(\"Hello!\")\n",
    "\n",
    "response = model.invoke([systemMessage, humanMessage])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact Implementation Starts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Leveraging `ChatPromptTemplate` and populating a conversation by setting the system instructions along with a interaction between Human and AI assistant. \n",
    "\n",
    "Keeping the placeholders inside the prompt template, making it reusable for different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['language', 'text']\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate # importing ChatPromptTemplate which is used to create a prompt template for chat models\n",
    "\n",
    "# creating a sample chat conversation prompt template with some messages\n",
    "# \"system\" message sets the context for the AI assistant\n",
    "# \"human\" message is the user's input\n",
    "# \"ai\" message is the expected response from the AI assistant\n",
    "# Syntax for defining messages is similar to a list of tuples with roles and content E.g., (\"role\", \"content\")\n",
    "\n",
    "prompt_template =  ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI assistant having knowledge in solving language translations.\"),\n",
    "        (\"human\", \"Hi! I want to translate {text} from English to {language}.\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# using the prompt template to format the input for the chat model\n",
    "# the format method replaces the placeholders {text} and {language} with actual values\n",
    "print(prompt_template.messages[1].input_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Replacing placeholders with the original values and formatting the prompt using `format_messages` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful AI assistant having knowledge in solving language translations.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hi! I want to translate I am doing good. from English to Telugu.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "formatted_messages = prompt_template.format_messages(text=\"I am doing good.\", language=\"Telugu\")\n",
    "\n",
    "print(formatted_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Pass the formatted prompt to the LLM and invoke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! The translation of \"I am doing good\" from English to Telugu can be:\n",
      "\n",
      "నేను బాగానే ఉన్నాను. (Nēnu bāgānē unnānu.)\n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "*   **నేను (Nēnu):** I\n",
      "*   **బాగానే (Bāgānē):** Well/Good\n",
      "*   **ఉన్నాను (Unnānu):** Am/Existing/Doing\n",
      "\n",
      "So, the whole sentence translates to \"I am well/good.\"\n"
     ]
    }
   ],
   "source": [
    "print(model.invoke(formatted_messages).content.strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
