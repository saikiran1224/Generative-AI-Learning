{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning from Human Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we recap the Generative AI lifecycle, in the \"Adapt and Align Model\" Section we have three sub-sections\n",
    "- Prompt Engineering *(Completed)*\n",
    "\n",
    "- Fine tuning *(Completed)*\n",
    "- Align with Human Feedback **(We are here)**\n",
    "\n",
    "We have tried both the ways Prompt Engineering and Fine-tuning to optimize our model responses and produce the outputs in a more human understandable way. However, there are cases while the model is behaving badly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models behaving badly\n",
    "\n",
    "Consider there are three cases, where the following questions are asked to the LLM model: \n",
    "\n",
    "- Asked to provide a Joke? - Replied with some non-related answer. **(Was it Helpful?)** \n",
    "\n",
    "- Asked some health related question - Replied with a known answer which the model should have refused. **(Was it Honest?)**\n",
    "\n",
    "- Asked some harmful or dangerous question - Replied with steps instead of refusing **(Was it Harmlessness?)** \n",
    "\n",
    "These imporant human values - Helpfulness, Honesty and Harmlessness are sometimes combinely called HHH and these are set of principles which guide developers in use of responsible AI.\n",
    "\n",
    "Additional fine-tuning with Human feedback helps to better align models with human preferences and helps in increasing the HHH values and helps in decreasing the toxicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How RLHF works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It was proved by the researchers that when the LLM model which was finetuned with RLFH has surpassed the performance of all other models like pre-trained model, Instruction fine-tuned models.\n",
    "\n",
    "- It uses the method of Reinforcement Learning Technique to fine-tune the model based on the human feedback. \n",
    "\n",
    "- The whole process in simple steps: \n",
    "\n",
    "    Instruct fine-tuned model -> RLHF -> Human feedback aligned LLM\n",
    "\n",
    "    As the name suggests the RLHF will help to maximize the helpfulness relevance, minimize harm and avoids dangerous topics.\n",
    "    \n",
    "- Models will learn the preference of each individual users through personal feedback. This will help in personalization of LLMs leading to a significant development of Personal AI Assistants, or Individualized learning plans. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RLFH Process](https://miro.medium.com/v2/resize:fit:1400/1*PK8BewZA0daezu-rZWm_Wg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://miro.medium.com/v2/resize:fit:1400/1*PK8BewZA0daezu-rZWm_Wg.png\">Image Credits</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Reinforcement Learning Technique, we have two main entities named \"Agent\" and \"Environment\". The Agent will be assigned with some RL policy and Environment is the place where the model will work. We also have action and state, and reward connected to each other. \n",
    "\n",
    "Similarly the above diagram depicts the Reinforcement Learning from Human Feedback in terms of LLM. The following are the steps involved: \n",
    "\n",
    "**Step 1:** Agent is our Instruct LLM, and will be assigned the LLM policy. \n",
    "\n",
    "**Step 2:** Once the agent (LLM) is provided with a prompt, then the action will be triggered and sent to the Environment (LLM Context).\n",
    "\n",
    "**Step 3:** Environment produces the response and it returns the current context state back to the Agent. \n",
    "\n",
    "**Step 4:** The crucial part here is there will be Human Observer or Reward model which evaluates the response generated by the LLM and checks if the response is less harmful, honest and helpful. Based on some score gets calculated and sent as form of \"Reward\" to the Agent.\n",
    "\n",
    "**Step 5:** Likewise this process repeats, so that LLM will learn from the Human feedback for each response it is generated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of the reward can lie between 0 and 1 which will be provided by the Human after keenly evaluating the response. Most often it is difficult for a human to manually evaluate the responses, hence we can also use \"Reward Model\" which is trained using Supervised Machine Learning technique aganist dataset of various prompts and completions. \n",
    "\n",
    "This Reward model will act like a human observer, and provides the output to the end users."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
