{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning from Human Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we recap the Generative AI lifecycle, in the \"Adapt and Align Model\" Section we have three sub-sections\n",
    "- Prompt Engineering *(Completed)*\n",
    "\n",
    "- Fine tuning *(Completed)*\n",
    "- Align with Human Feedback **(We are here)**\n",
    "\n",
    "We have tried both the ways Prompt Engineering and Fine-tuning to optimize our model responses and produce the outputs in a more human understandable way. However, there are cases while the model is behaving badly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models behaving badly\n",
    "\n",
    "Consider there are three cases, where the following questions are asked to the LLM model: \n",
    "\n",
    "- Asked to provide a Joke? - Replied with some non-related answer. **(Was it Helpful?)** \n",
    "\n",
    "- Asked some health related question - Replied with a known answer which the model should have refused. **(Was it Honest?)**\n",
    "\n",
    "- Asked some harmful or dangerous question - Replied with steps instead of refusing **(Was it Harmlessness?)** \n",
    "\n",
    "These imporant human values - Helpfulness, Honesty and Harmlessness are sometimes combinely called HHH and these are set of principles which guide developers in use of responsible AI.\n",
    "\n",
    "Additional fine-tuning with Human feedback helps to better align models with human preferences and helps in increasing the HHH values and helps in decreasing the toxicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How RLHF works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It was proved by the researchers that when the LLM model which was finetuned with RLFH has surpassed the performance of all other models like pre-trained model, Instruction fine-tuned models.\n",
    "\n",
    "- It uses the method of Reinforcement Learning Technique to fine-tune the model based on the human feedback. \n",
    "\n",
    "- The whole process in simple steps: \n",
    "\n",
    "    Instruct fine-tuned model -> RLHF -> Human feedback aligned LLM\n",
    "\n",
    "    As the name suggests the RLHF will help to maximize the helpfulness relevance, minimize harm and avoids dangerous topics.\n",
    "    \n",
    "- Models will learn the preference of each individual users through personal feedback. This helps in "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
