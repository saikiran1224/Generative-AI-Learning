{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Efficient Fine-tuning (PEFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we know in Full Fine-tuning, all the parameters of the LLM model gets updated and a new version gets released. But in PEFT, it only updates a small set of parameters. \n",
    "\n",
    "- Based on the chosen technique, sometimes the underlying parameters might get updated or else, original parameters gets freezed and very small set of parameters gets updated or a new layer will get added. \n",
    "\n",
    "- PEFT can be performed on a single GPU unlike Full fine-tuning requiring the same compute of pre-training the LLMs. \n",
    "\n",
    "- This solves the Catastrophic forgetting problem as some of zero paramters of the LLM are getting updated. \n",
    "\n",
    "- PEFT saves the memory and very flexible. It is unlike full fine-tuning where a new version of model gets released post fine-tuning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tradeoffs of PEFT \n",
    "\n",
    "- Parameter Efficiency \n",
    "\n",
    "- Improved Model peformance with less inference costs\n",
    "\n",
    "- Reduction in Training speed\n",
    "\n",
    "- Memory Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniques of PEFT\n",
    "\n",
    "There are three main techniques of Parameter Efficient Fine-tuning (PEFT). They are as mentioned below: \n",
    "\n",
    "### 1. Selective \n",
    "\n",
    "Only 'select' subset parameters of the LLM gets updated for fine-tuning and freezes the original LLM parameters.\n",
    "\n",
    "### 2. Additive\n",
    "\n",
    "We will add trainable layers on top of the LLM model without disturbing the model's original parameters.  There are two ways which we can add:\n",
    "\n",
    "- Adapters - adds a new layer\n",
    "\n",
    "- Soft Prompts - focuses on the input prompt also called as Prompt Tuning (remember not Prompt Engineering).\n",
    "\n",
    "### 3. Reparameterization\n",
    "\n",
    "Leverage LoRA (Low Rank Adaption) Technique and re-parameterize the model weights during training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA (Low Rank Adaption of LLMs) - Reparameterization Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Aims to reduce the number of parameters that need to be updated during fine-tuning, making the process more parameter-efficient.\n",
    "\n",
    "- During fine-tuning, the Model weights are reparameterized using a low-rank factorization. This means that the original weight matrix is decomposed into two smaller matrices, one with lower rank making the parameters that are to be updated during fine-tuning much smaller in size.\n",
    "\n",
    "- The low-rank matrices are then used to reconstruct the original weight matrix, allowing the model to maintain its performance while only updating a fraction of the parameters during fine-tuning.\n",
    "\n",
    "- During fine-tuning, the low-rank reparameterization allows the model to adapt to new tasks while preserving the knowledge learned during pre-training by avoiding catastrophic forgetting.\n",
    "\n",
    "- This is particularly useful for large language models (LLMs) where the number of parameters can be in the billions, making full fine-tuning computationally expensive and time-consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Tuning - Additive Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LoRA was a technique, where in which we have used an efficient way of updating the model parameters without re-training each and every single parameter again. \n",
    "\n",
    "- Prompt Tuning is a technique where we will add a new layer to the existing LLM without changing the weights at all. \n",
    "\n",
    "### Is it related to Prompt Engineering? \n",
    "\n",
    "- It looks like its related to \"Prompt Engineering\" but this technique is completely different. Prompt Engineering means refining our prompt, using one or few-shot inference. \n",
    "\n",
    "- But there are drawbacks for the same where we require lot of manual efforts to write the prompts, and the context window limitation when we use one or few shot inference.\n",
    "\n",
    "### What is Prompt Tuning? \n",
    "\n",
    "- Prompt Tuning is a technique where we add a new set of layer consisting of additional trainable tokens called as Soft Prompts. \n",
    "\n",
    "- During the supervised learning process, these trainable tokens will be assigned an optimal value.\n",
    "\n",
    "- These set of trainable tokens called as Soft Prompts, are the virtual tokens comprising of the same size as the embedding vectors. \n",
    "\n",
    "- These soft prompts are not fixed for some words, instead these can be trained with any set of words like virtual tokens. During the supervised learning, these virtual tokens will be trained and assigned some value. \n",
    "\n",
    "- In Full fine-tuning, all the model parameter weights are updated, whereas in Prompt Tuning the original LLM weights are left untouched and a new layer is added. Only these embedding vectors of the soft prompts gets updated over time to optimize the model performance. \n",
    "\n",
    "- We can train different set of soft prompts for different tasks, and we can change therm during the time of inference. We can write our input prompt to change the soft prompts. \n",
    "\n",
    "- These Soft prompts (trainable tokens) occupies very less size on disk making it a powerful fine-tuning strategy."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
