{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks of LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the BLUE Score and ROUGE metrics are able to evaluate the text translation and summarization, they still be remained or considered as Simple Evaluation Metrics. \n",
    "\n",
    "To identify the complete performance of the LLM, we need to train the model using some good datasets based on some pre-defined standards as mentioned by Researchers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Bencharks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three main Benchmarks that can be used to evaluate the performance of a model. These are: \n",
    "\n",
    "1. SuperGLUE\n",
    "\n",
    "2. GLUE\n",
    "3. MMLU \n",
    "4. HELM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. GLUE (General Language Understading Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GLUE is introduced in the year of 2018.\n",
    "\n",
    "- It is a collectio of two main natural language processing tasks such as \"Sentiment Analysis\" and \"Question and Answering (QA)\".\n",
    "\n",
    "- We can use GLUE as a benchmark to test, compare and evalulate the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SuperGLUE \n",
    "\n",
    "- This is introduced as a Successor of GLUE by solving the drawbacks identified in its predeccessor. \n",
    "\n",
    "- In addition to the tasks which are evaluated by GLUE, SuperGLUE supports evaluation of \"Mult-Sentence reasoning\" and \"Reading comprehension\". \n",
    "\n",
    "> As the LLM models size gets larger enough, the benchmarks such as SuperGLUE will start to match the human ability on certain tasks. It means the Models are equally working or responding as Human does. But still subjectively, sometimes they won't match. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. MMLU (Massive Multitask Language Understanding)\n",
    "\n",
    "- As the LLM models are evolving daily and as they are getting more bigger, MMLU is introduced to work with the modern LLM models. \n",
    "\n",
    "- The LLM model is tasted way more than normal human ability such as law, computer science, history, US law, etc. and also evalulates the performance on 204 tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. HELM (Holistic Evaluation of Language Models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This HELM framework helps in improving the transparency of the models, mentioning which LLM model is suitable for which task. \n",
    "\n",
    "- HELM follows multi-metric approach where it evaluates the model by measuring seven metrics across 16 core scenarios making the models to expose clearly on various tasks and will be able to easily identify the trade-offs between the models. \n",
    "\n",
    "- This framework also helps in evaluating the fairness, toxicity, bias and much more instead of calculating the traditional precision and F1 score. \n",
    "\n",
    "- The HELM framework is continously evolving and a living benchmark which gets updated with the latest metrics, scenarios and models. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
