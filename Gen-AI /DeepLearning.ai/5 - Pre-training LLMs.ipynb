{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training LLMs\n",
    "\n",
    "Let us understand how the LLMs that we are currently using are trained. We will also have a look on the memory and compute related details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Process of LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of training LLMs from scratch is termed as **Pre-training**. \n",
    "\n",
    "- Vast amount of data ranging from Gigabytes, Terabytes to Petabytes (GB - TB - PB) of data is gathered which is mostly in unstructured format for training the LLM model.\n",
    "\n",
    "- This data will be used for training the LLMs, during the training LLMs will learn the patterns in the provided data. \n",
    "\n",
    "- The model weights are optimized during the training processes and loss function is used to minimize the loss. \n",
    "\n",
    "- The training data needs to be optimized and cleaned first. Training data will be converted into Tokens assigning IDs to each and Embeddings are created. \n",
    "\n",
    "> Pre-training requires large amount of compute and memory for the model to be perfectly trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA: Out of Memory Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When loading the models on NVIDIA GPU, mostly we receive a error as mentioned \"CUDA: Out of Memory Error\". \n",
    "\n",
    "CUDA stands for Compute Unified Device Architecture. CUDA plays a major role in helping Deep Learning models trained. \n",
    "\n",
    "Why we face this error? Due to the large size of LLM, the CUDA memory gets exhausted and hence this error is thrown. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So we have the problem now, LLM models are taking large amount of memory and due to this we are not able to leverage CUDA which plays a major role in training our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Parameters and Memory in terms of LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we already know the higher the model parameter size, the efficient and powerful the LLM model. \n",
    "\n",
    "Consider \n",
    "\n",
    "1 Parameter = 4 Bytes of memory,\n",
    "1 Billion parameters = 4 * 1 Billion Bytes = 4 * 10^9 Bytes = 4 GB @ 32-bit. \n",
    "\n",
    "Additionally, along with each parameter we need Adam Optimizer, Gradients and Activations at temporary memory which might take 20 bytes of extra memory. \n",
    "\n",
    "Total Size for per parameter = 4 bytes + 20 bytes = 24 bytes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Calculations \n",
    "\n",
    "Memory needed to Store the model - 4 GB @ 32-bit full precision\n",
    "\n",
    "Memory needed to Train the model - 24 GB @ 32-bit full precision \n",
    "\n",
    "So, we need 24 GB of GPU RAM to train such model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniques to reduce Memory Consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization\n",
    "\n",
    "It is a technique where we will reduce the memory where our weights are stored. Usually, all the weights are stored in FP32 format (Floating point 32-bit). \n",
    "\n",
    "As part of Quantization, we are going to make it reduce till FP16 (16-bit). This way, we are reducing nearly 50% storage. \n",
    "\n",
    "> Do you know? BFLOAT16 is a latest technique developed by Google Brain. FLAN-T5 model is pre-trained on BFLOAT16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals of Optimization\n",
    "\n",
    "- Reduces memory required to store the weights and for training the models.\n",
    "\n",
    "- Projects original 32-bit floating point to lower precision values.\n",
    "- Quantization aware training (QAT) learns the quantization scaling factors during training. Modern deep learning libraries are aware of the QAT. \n",
    "- Let us have a look on the below example where the model remains same, but how it varied after doing quantization. \n",
    "\n",
    "    | Model | Model | Model |\n",
    "    | :--: | :--: | :--: |\n",
    "    | 4GB of GPU RAM @ 32-bit precision |  2GB of GPU RAM @ 16-bit precision |  1GB of GPU RAM @ 8-bit precision | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
