{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training LLMs\n",
    "\n",
    "Let us understand how the LLMs that we are currently using are trained. We will also have a look on the memory and compute related details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Process of LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of training LLMs from scratch is termed as **Pre-training**. \n",
    "\n",
    "- Vast amount of data ranging from Gigabytes, Terabytes to Petabytes (GB - TB - PB) of data is gathered which is mostly in unstructured format for training the LLM model.\n",
    "\n",
    "- This data will be used for training the LLMs, during the training LLMs will learn the patterns in the provided data. \n",
    "\n",
    "- The model weights are optimized during the training processes and loss function is used to minimize the loss. \n",
    "\n",
    "- The training data needs to be optimized and cleaned first. Training data will be converted into Tokens assigning IDs to each and Embeddings are created. \n",
    "\n",
    "> Pre-training requires large amount of compute and memory for the model to be perfectly trained."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
