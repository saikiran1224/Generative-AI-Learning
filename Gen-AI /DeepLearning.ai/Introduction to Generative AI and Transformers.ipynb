{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative AI and LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative AI is a subset of traditional machine learning and the machine learning models that underpin generative AI have learned these abilities by finding statistical patterns in massive datasets of content that was originally generated by humans. \n",
    "\n",
    "Large language models have been trained on trillions of words over many weeks and months, and with large amounts of compute power. \n",
    "\n",
    "**Foundation models**, also known as Base models identified with billions of parameters, exhibit emergent properties beyond language alone, and researchers are unlocking their ability to break down complex tasks, reason, and problem solve. \n",
    "\n",
    "**Parameters** define the size of the model. There are various types of models with various sizes. Consider Parameters are like the connection between one neuron to another neuron. These are weights which are set during the training of the model. To simplify, the higher the parameters of the mdoel the efficient and advanced the LLM model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation before Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier, before the evolution of Transformer Architecture, we have the traditional models such as Recuurent Neural Networkds (RNNs) and Convolutional Neural Networks(CNNs). \n",
    "\n",
    "Once the input sentence is provided, their main role is role is to read and understand the information and predict the next word. But it all depends on the context or the understanding of the previous words, if the model is not able to understand the previous words, or due to might be ambiguous meaning it may predict some Wrong word. This is where these models were failed. \n",
    "\n",
    "Then during 2017, \"Attention is all you need\" a research paper is published by Google researchers, from University of Toronto which is responsible for the current evolution of LLMs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Transformer Architecture in brief"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We all know Transformers are the core basement or foundation of building of LLMs, but it is also very important to know the working or the architecture how these are developed.\n",
    "\n",
    "Below is the Transformer Architecture\n",
    "\n",
    "![Transformer Architecture](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/idP8oaT2QPK8TAdEt6WYEg_41e77abe4ea6457e9706e0c4fa379ff1_image.png?expiry=1745280000000&hmac=eQFFF9f9vaB9KI86HhbiIv3yljLjzPI0fV11lyIrkoE)\n",
    "\n",
    "<a href=\"https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/idP8oaT2QPK8TAdEt6WYEg_41e77abe4ea6457e9706e0c4fa379ff1_image.png?expiry=1745280000000&hmac=eQFFF9f9vaB9KI86HhbiIv3yljLjzPI0fV11lyIrkoE\">Source</a>\n",
    "\n",
    "Transformers consists of two main components: \n",
    "\n",
    "1. Encoder\n",
    "\n",
    "2. Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components in Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we receive the input sentence which is in the form of string, it cant be directly passed to the Encoder as machines does calculations on numbers on not on Words or sentences. \n",
    "\n",
    "Initially, we will convert the given input into a set of embeddings or vectors using some embedding models. Next step, we will combine the same with Positional Encoding which helps us to determine the position of the word in the input sentence.\n",
    "\n",
    "These will be passed to our **Self-Attention layer** where the \"Attention Map\" gets calculated. Its like the model reads the input sentence, and calculates the weight and will understand how each word is related to another words or how important/relate to the other words. Similarly, there will multiple calculations done in different ways, hence it is called \"Multi-headed Self-Attention layer\".\n",
    "\n",
    "Next, this will be sent to the Feed Forward Neural Network which provides logits as the output and gets sent to the Decoder. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components in Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here also from the start, each word in the input sentene (shift right) each word gets sent combination of positional embedding and word embedding and passed to the Masked Multi-head Self attention layer, and then the output received from Encoder, will be again passed on to the Feed forward Neural Network.\n",
    "\n",
    "The output of the Feed forward Neural Network will be sent to the **Softmax** layer which consists of the probabilities. For example, the output here will be a big dictionary consists of all the training words which are associated with their corresponding probabilities. \n",
    "\n",
    "The Transformer predicts which word has highest probability and that will be sent as an output from the Softmax layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
